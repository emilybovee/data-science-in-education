[
["introduction.html", "Data Science in Education Chapter 1 Introduction", " Data Science in Education multiple 2019-06-04 Chapter 1 Introduction Dear Data Scientists, Educators, and Data Scientists who are Educators: This book is a warm welcome and an invitation. If you’re a data scientist in education or an educator in data science, we know that your role isn’t exactly straightforward. We welcome everyone who wants to understand data science in education better. If you work in education or data science, you also own a part of the solution. We invite everyone to help define what it means to practice data science in education by sharing their experiences. 1.0.1 The Challenge of Data Science in Education We’ll get to work on understanding data science in education soon, but first let’s talk about why this relationship is not such a straightforward thing. Talking about data science in education is hard because everyone tackles it on different levels. If education were a building, it would be multi-storied with many rooms. There are privately and publicly funded schools. There are more than eighteen possible grade levels. You can be educated alone in front of a computer or with others in a classroom. This imaginary building also has rooms the residents never see: Business and finance staff plan for efficient use of limited funds. The transportation department plans bus routes across vast spaces. University administrators search for the best way to measure career readiness. So why don’t we see more data science happening in these areas of education? Data science is a relatively new field. This means that our community is still trying to work out how it all fits in. It also means that folks in education aren’t always used to having someone around who understands education, knows how to code, and can use statistical techniques all at once. 1.0.2 Meeting the Challenge As the data science field grows, we’ll need better language to describe what it means in education and how to use it to meet our goals for students. In this book we want to take a step towards understanding data science in education better by exploring challenges you’re likely to encounter no matter how you work with data in education. After that we describe basic and advanced data science skills that you can use to tackle these challenges. Finally, we’ll present walkthroughs of analyses conducted in the education setting to bring these challenges and techniques to life. We hope after reading this book you’ll feel like you’re not alone in defining how to do data science in your education job. We also hope the techniques and examples here give you ideas to kickstart using data science to meet your goals in education. Finally, we hope you accept our invitation to contribute to this work by sharing your own challenges and solutions. "],
["how-to-use-this-book.html", "Chapter 2 How to Use This Book", " Chapter 2 How to Use This Book It is really hard to draw clean boundaries around the topic of data science in education because people are educated in all kinds of settings and in all kinds of age groups. Education organizations require different kinds of staff to make it work, which means different kinds of data science uses. A teacher’s approach to learning from data is different from an administrator’s or an opeartions manager. Since there are many different readers, we believe there should be different ways to use the book, both as a reader and as a contributor. Here are some ways to use this book: Read the Book Cover to Cover Reading the book all the way through will give a nice high level view of data science in education, starting from the unique challenges of using data science in education and ending with code for example analyses. Pick a Chapter That is Useful for Your Level of Experience and Start There If you are a student or if you work in education, you may be trying to solve a very specific problem with data, like analyzing student quiz scores, projecting classroom sizes, or pitching a new data analysis method. In this case it might be useful to jump ahead to a chapter or section that discusses your area of interest. Read Through the Walkthroughs and Run the Code If you are here to learn and practice coding in R, you can work through the example analyses. We wrote these based on typical data problems you might find as a student or staff in education, so it is worthwhile to copy or type the code, run it in your console, and change it to experiment with the results. Contribute to the Book We quickly learned when planning the book that there are many ways to approach this topic and still we wanted to write in a way that is directly useful and practical for our readers in education. One way to meet this goal is to build procedures into the work for readers to directly contribute. We hope that as the book evolves it grows to reflect the observable needs of data scientists in education. Here are some ways readers can contribute: Submit a pull request to our GitHub site that describes a data science problem that is unique to the education setting Submit a pull request to share a solution for the problems discussed in the book to the education setting Share an anonymized dataset What is a Data Scientist in Education? One way to define data science is to think of it as combining three skills to do data analysis: programming, statistics, and content knowledge. Though if you Google “what is a data scientist” you’ll won’t find a simple answer. But for this book’s exploration, thinking of data science as a combination of these three skills is useful because we can try substituting the field of education in for “content knowledge.” Even then, we still face a broad field of possibilities when imagining what a data scientist in education actually does on a day-to-day basis. While having no established data science identity makes it hard for educators to explain their data work to the layperson, it does allow them to take on a variety of data-related activities and, ultimately, build the definition of the role. So rather than grapple with defining this role, let’s share some examples of what data scientists do in the field of education. Leading Office Culture Toward a Data-Driven Approach Jesse, a director at an education non-profit in Texas, is setting up a database to house student achievement data. This project requires a number of data science skills we’ll discuss in chapter five, including cleaning data into a consistent format. Once the data is prepared, Jesse builds dashboards to help her teammates explore the data. But not all of Jesse’s work can be found in a how-to manual for data scientists. She manages a team and serves as the de facto project manager for IT initiatives. And given her expertise and experience in data science, she’s leading the charge towards a more data-driven approach within the organization. Helping School Districts Plan to Meet Their Goals Ryan, a special education administrator in California, uses data science to reproduce the state department of education’s special education compliance metrics, then uses the results to build an early warning system for compliance based on local datasets. In this case, Ryan uses foundational data science skills like data cleaning, visualization, and modeling to help school districts monitor and meet their compliance requirements. Doing and Empowering Research On Data Scienctists in Education Joshua, Assistant Professor of STEM Education at University of Tennessee in Knoxville, researches how students do data science and helps teachers teach the next generation of data-informed citizens. He makes this work possible by building R packages—self-contained groups of data tools—that he and other researchers use to analyze datasets efficiently. The data scientists in these examples apply statistics and programming to create new knowledge in the education field. But that’s as far as we can go when looking for commonalities in their day-to-day work. Maybe the education community will develop common norms and expectations for how it all works together as the relationship between data science and education grows. But because this relationship is still young, it is important that the people growing data science within education understand the culture and unique challenges in their education job. Afterall, the defining feature that will differentiate data science in education from data science in general will be doing data science that meets the unique needs of students, staff, and administration in education. Because data science in the school setting is a relatively new phenomena, it’s understandable that school staff may be wary of how data is collected and analyzed. It’s common for school staff to question how data is used, particularly if the data is used to describe staff and student performance. School systems that want to evolve their data analysis processes into something practical and meaningful to student progress will need to do the difficult work of addressing these worries. "],
["a-reproducible-approach.html", "Chapter 3 A Reproducible Approach", " Chapter 3 A Reproducible Approach One way to do this is to build analytic processes that are open about what data is collected, how it is collected, how it is analyzed, and how it is considered alongside other data when used in decision-making conversations. This can be achieved through a number of activities, including regular conversations about analytic methods, written reports describing data collection, and receiving input about analytic goals from staff members. One such process for achieiving openess in data collection and analysis is called reproducible research. The concept of Reproducible work is the idea that a completed analysis should come with all the necessary materials, including a description of methodology and programming code, needed for someone else to run the analysis and achieve the same results. If school staff are apprehensive about how school data is collected and used, it follows that a more transparent method for using data could go some way towards putting school staff–the consumers of school data–at ease. "],
["a-self-driven-analytic-approach.html", "Chapter 4 A Self-Driven Analytic Approach", " Chapter 4 A Self-Driven Analytic Approach An organization should encourage their staff to do their own data analyses primarily for the purpose of testing their own hypotheses about student learning in their classrooms and to directly guide decisions about how they deliver instruction. There are at least two benefits to this approach. First, staff begin to realize the value of doing data analysis as an ongoing inquiry into their outcomes, instead of a special event once a year ahead of school board presentations. Second–and more important for the idea of reducing apprehension around data analysis in schools–school staff begin to demystify data analysis as a process. When school staff collect and analyze their own data, they know exactly how it is collected and exactly how it is analyzed. The long-term effect of this self-driven analytic approach might be more openess to analysis, whether it is self-driven or conducted by the school district. Building and establishing data governance that advocates for an open and transparent analytic process is difficult and long-term work, but the result will be less apprehension about how data is used and more channels for school staff to participate in the analysis. Here are more practical steps a school district can take towards building a more open approach to analysis: Make technical write-ups available so interested parties can learn more about how data was collected and analyzed Make datasets available to staff within the organization, to the extent that privacy laws and policies allow Establish an expectation that analysts present their work in a way that is accessible to many levels of data experience Hold regular forums to discuss how the organization collects and uses data "],
["unique-challenges.html", "Chapter 5 Unique challenges 5.1 Challenges common to doing data science in any domain 5.2 Lack of processes and procedures 5.3 Few guidelines from research and evaluation 5.4 Limited training and educational opportunities for educational data science 5.5 The complex and messy nature of educational data 5.6 Ethical and legal concerns 5.7 Analytic challenges", " Chapter 5 Unique challenges Educational data science is a new domain. It presents opportunities, like those discussed in the previous chapter, but also some challenges. These challenges vary a lot: We consider doing data science in education to include not only access, processing, and modeling data, but also social and cultural factors, like the training and support that educational data scientists have available to them. These challenges, then, range from the very general (and common to all domains in which data science is carried out) to very particular to educational data science. These are discussed in the remainder of this chapter. 5.1 Challenges common to doing data science in any domain One challenge for educational data scientists is common to data scientists in other domains: Combining content knowledge, programming, and statistics to solve problems is a fairly new idea. In particular, the amount of data now available means that programming is often not only helpful, but necessary, for stakeholders to use data in education. Programming is powerful, but challenging; many of us in education do not have prior experience with it. Despite this challenge and the difficulty of writing the first few lines of code, there is good evidence, and many examples, that even those of us without prior programming experience can learn. 5.2 Lack of processes and procedures Other challenges are more about the process and practice of doing educational data science. Education is a field that is rich with data: survey, assessment, written, and policy and evaluation data, just for a few examples. Nevertheless, sometimes, there is a lack of processes and procedures in place for school districts and those working in them to share data with each other in order to build knowledge and context. Moreover, in academic and research settings, there are not often structures in place to facilitate the analysis of data and sharing of results. 5.3 Few guidelines from research and evaluation While there is a body of past research on students’ work with data (see Lee &amp; Wilkerson, 2018, for a review), there is limited information from case- or design-based research on how others–teachers, administrators, and data scientists–use data in their work. In other words, we do not have a good idea for what best practices in our field are. This challenge is reflected in part in the variability in the roles of those who work with data. Many districts employ data analysts and research associates; some are now advertising and hiring for data scientist positions. 5.4 Limited training and educational opportunities for educational data science Educational data science is new. At the present time, there are limited opportunities for those working in education to build their capabilities in educational data science (though this is changing to an extent; see Anderson and colleagues’ work to create an educational data science certificate program at the University of Oregon and Bakers’ educational data mining Massive Open Online Course offered through Coursera). Many educational data scientists have been trained in fields other than statistics, business analytics, or research. Moreover, the training in terms of particular tools and approaches that educational data scientists are highly varied. 5.5 The complex and messy nature of educational data Another challenge concerns the particular nature of educational data. Educational data are often hierarchical, in that data at multiple “levels” is collected. These levels include classrooms, schools, districts, states, and countries - quite the hierarchy! In addition to the hierarchical nature of educational data, by their nature, these data often require linking with other data, such as data that provides context at each of the aforementioned levels. For example, when data is collected on students at the school level, it is often important to know about the training of the teachers in the school; data at the district level needs to be interpreted in the context of the funding provided by the community in terms of per-pupil spending and other, for example. A final aspect concerns the type of data collected. Often, educational data is numeric, but just as often, it is not: It involves characteristics of students, teachers, and other individuals that are categorical; open-ended responses that are strings; or even recordings that consist of audio and video data. All of these present challenges to the educational data scientist. 5.6 Ethical and legal concerns Related to the complex and messy nature of educational data is its confidential nature. At the K-12 level, most data requires protections because of its human subjects focus, particularly because the data is about a protected population, youth. A closely related issue concerns the aims of education. Those working in education often seek to improve it and often work to do so with a scarcity of school and community resources. These ethical, legal, and even values-related concerns may become amplified as the role of data in education increases. They should be carefully considered and emphasized from the outset by those involved in educational data science. 5.7 Analytic challenges Due to the challenging nature of educational data, analyzing educational data is hard, too. The data is often not ready to be used: It may be in a format that is difficult to open without specialized software or it may need to be “cleaned” before it is usable. Closely related to the ethical and legal challenges, educational data scientists should be conscious of potential racial and gender biases in school models, and challenge not reinforce them. Because of the different types of data, the educational data scientist must often use a variety of analytic approaches, such as multi-level models, models for longitudinal data, or even models and analytic approaches for text data. "],
["foundational-skills.html", "Chapter 6 Foundational Skills 6.1 Track One: Getting Started 6.2 Downloading R and R Studio 6.3 Check that it worked 6.4 Help, I’m completely new to using R / R Studio! 6.5 Creating Projects 6.6 Packages 6.7 Loading Data from Various Sources 6.8 Processing Data 6.9 Communicating / sharing results 6.10 Other foundational notes", " Chapter 6 Foundational Skills This chapter is organized into two tracks (though, of course, you are welcome to read both). If you have experience using R - or have used it a few times, attended a workshop, or been involved with a collaborator who used it - consider starting with Track Two, focused on ‘data loading and manipulation using the tidyverse’, which covers reading/saving files, pipes, selecting, filtering, etc. chapters. Otherwise, start at Track One, which covers installation, projects, and packages–and then proceed to the second track. 6.1 Track One: Getting Started First, you will need to download the latest versions of R and R Studio. R is a free environment for statistical computing and graphics using the programming language R. R Studio is a set of integrated tools that allows for a more user-friendly experience for using R. Although you will likely use R Studio as your main console and editor, you must first install R as R Studio uses R behind-the-scenes. Both are freely-available, cross-platform, and open-source. 6.2 Downloading R and R Studio 6.2.1 To download R: Visit this page to download R: https://cran.r-project.org/ Find your operating system (Mac, Windows, or Linux) Download the ‘latest release’ on the page for your operating system and download and install the application Don’t worry; you will not mess anything up if you download (or even install!) the wrong file. Once you’ve installed both, you can get started. 6.2.2 To download R Studio: Visit this page to download R studio: https://www.rstudio.com/products/rstudio/download/ Find your operating system (Mac, Windows, or Linux) Download the ‘latest release’ on the page for your operating system and download and install the application If you do have issues, consider this page, and then reach out for help. One good place to start is the R Studio Community is a great place to start. 6.3 Check that it worked Open R Studio. Find the console window and type in 2 + 2. If what you can guess is returned (hint: it’s what you expect!), then R Studio and R both work. 6.4 Help, I’m completely new to using R / R Studio! If you’re completely new, Swirl is a great place to start, as it helps you to learn R from within R Studio. Visit this page to see some directions: http://swirlstats.com. And if you’re ready to go, please proceed to the next sections on processing and preparing, plotting, loading, and modeling data and sharing results. 6.5 Creating Projects Before proceeding, we’re going to take a few steps to set ourselves to make the analysis easier; namely, through the use of Projects, an R Studio-specific organizational tool. To create a project, in R Studio, navigate to “File” and then “New Directory”. Then, click “New Project”. Choose a directory name for the project that helps you to remember that this is a project that involves data science in education; it can be convenient if the name is typed in lower-case-letters-separated-by-dashes, like that. You can also choose the sub-directory. If you are just using this to learn and to test out creating a project, you may consider placing it in your downloads or another temporary directory so that you remember to remove it later. Even if you do not create a Project, you can always check where your working directory (i.e., where your R is pointing) is by running getwd(). To change it manually, run setwd(desired/file/path/here). 6.6 Packages “Packages” are shareable collections of R code that provide functions (i.e., a command to perform a specific task), data and documentation,. Packages increase the functionality of R by improving and expanding on base R (basic R functions). 6.6.1 Installing and Loading Packages To download a package, you must call install.packages(): install.packages(&quot;dplyr&quot;, repos = &quot;http://cran.us.r-project.org&quot;) You can also navigate to the Packages pane, and then click “Install”, which will work the same as the line of code above. This is a way to install a package using code or part of the R Studio interface. Usually, writing code is a bit quicker, but using the interface can be very useful and complimentary to use of code. After the package is installed, it must be loaded into your R Studio session using library(): library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union We only have to install a package once, but to use it, we have to load it each time we start a new R session. a package is a like a book, a library is like a library; you use library() to check a package out of the library - Hadley Wickham, Chief Scientist, R Studio 6.6.2 Running Functions from Packages Once you have loaded the package in your session, you can run the functions that are contained within that package. To find a list of all those functions, you can run this in the R Studio console: help(package = dplyr) The documentation should tell you what the function does, what arguments (i.e., details) needed for it to successfully run, examples, and what the output should look like. If you know the specific function that you want to look up, you can run this in the R Studio console: ??dplyr::filter Once you know what you want to do with the function, you can run it in your code: dat &lt;- # example data frame data.frame(stringsAsFactors=FALSE, letter = c(&quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;), number = c(1L, 2L, 3L, 4L, 5L)) dat ## letter number ## 1 A 1 ## 2 A 2 ## 3 A 3 ## 4 B 4 ## 5 B 5 filter(dat, letter == &quot;A&quot;) # using dplyr::filter ## letter number ## 1 A 1 ## 2 A 2 ## 3 A 3 6.6.3 Track Two: Welcome to the Tidyverse The Tidyverse is a set of packages for data manipulation, exploration, and visualization using the design philosophy of ‘tidy’ data. Tidy data has a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. The packages contained in the Tidyverse provide useful functions that augment base R functionality. You can installing and load the complete Tidyverse with: install.packages(&quot;tidyverse&quot;) library(tidyverse) For more information on tidy data, check out Hadley Wickhams’s Tidy Data paper. 6.7 Loading Data from Various Sources In this section, we’ll load data. You might be thinking that an Excel file is the first that we would load, but there happens to be a format which you can open and edit in Excel that is even easier to use between Excel and R as well as SPSS and other statistical software, like MPlus, and even other programming languages, like Python. That format is CSV, or a comma-separated-values file. The CSV file is useful because you can open it with Excel and save Excel files as CSV files. Additionally, and as its name indicates, a CSV file is rows of a spreadsheet with the columns separated by commas, so you can view it in a text editor, like TextEdit for Macintosh, as well. Not surprisingly, Google Sheets easily converts CSV files into a Sheet, and also easily saves Sheets as CSV files. For these reasons, we start with - and emphasize - reading CSV files. 6.7.1 Saving a File from the Web You’ll need to copy this URL: https://goo.gl/bUeMhV Here’s what it resolves to (it’s a CSV file): https://raw.githubusercontent.com/data-edu/data-science-in-education/master/data/pisaUSA15/stu-quest.csv This next chunk of code downloads the file to your working directory. Run this to download it so in the next step you can read it into R. As a note: There are ways to read the file directory (from the web) into R. Also, of course, you could do what the next (two) lines of code do manually: Feel free to open the file in your browser and to save it to your computer (you should be able to ‘right’ or ‘control’ click the page to save it as a text file with a CSV extension). student_responses_url &lt;- &quot;https://goo.gl/bUeMhV&quot; student_responses_file_name &lt;- paste0(getwd(), &quot;/data/student-responses-data.csv&quot;) download.file( url = student_responses_url, destfile = student_responses_file_name) It may take a few seconds to download as it’s around 20 MB. The process above involves many core data science ideas and ideas from programming/coding. We will walk through them step-by-step. The character string &quot;https://goo.gl/wPmujv&quot; is being saved to an object called student_responses_url. student_responses_url &lt;- &quot;https://goo.gl/bUeMhV&quot; We concatenate your working directory file path to the desired file name for the CSV using a function called paste0. This is stored in another object called student_reponses_file_name. This creates a file name with a file path in your working directory and it saves the file in the folder that you are working in. student_responses_file_name &lt;- paste0(getwd(), &quot;/data/student-responses-data.csv&quot;) The student_responses_url object is passed to the url argument of the function called download.file() along with student_responses_file_name, which is passed to the destfile argument. In short, the download.file() function needs to know - where the file is coming from (which you tell it through the url) argument and - where the file will be saved (which you tell it through the destfile argument). download.file( url = student_responses_url, destfile = student_responses_file_name) Understanding how R is working in these terms can be helpful for troubleshooting and reaching out for help. It also helps you to use functions that you have never used before because you are familiar with how some functions work. Now, in R Studio, you should see the downloaded file in the Files tab. This should be the case if you created a project with R Studio; if not, it should be whatever your working directory is set to. If the file is there, great. If things are not working, consider downloading the file in the manual way and then move it into the directory that the R Project you created it. 6.7.2 Loading a CSV File Okay, we’re ready to go. The easiest way to read a CSV file is with the function read_csv() from the package readr, which is contained within the Tidyverse. Let’s load the tidyverse library: library(tidyverse) # so tidyverse packages can be used for analysis You may have noticed the hash symbol after the code that says library(tidyverse). It reads# so tidyverse packages can be used for analysis`. That is a comment and the code after it (but not before it) is not run (the code before it runs just like normal). Comments are useful for showing why a line of code does what it does. After loading the tidyverse packages, we can now load a file. We are going to call the data student_responses: # readr::write_csv(pisaUSA15::stu_quest, here::here(&quot;data&quot;, &quot;pisaUSA15&quot;, &quot;stu_quest.csv&quot;)) student_responses &lt;- read_csv(&quot;./data/student-responses-data.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_double(), ## CNT = col_character(), ## CYC = col_character(), ## NatCen = col_character(), ## STRATUM = col_character(), ## Option_Read = col_character(), ## Option_Math = col_character(), ## ST011D17TA = col_character(), ## ST011D18TA = col_character(), ## ST011D19TA = col_character(), ## ST124Q01TA = col_logical(), ## IC001Q01TA = col_logical(), ## IC001Q02TA = col_logical(), ## IC001Q03TA = col_logical(), ## IC001Q04TA = col_logical(), ## IC001Q05TA = col_logical(), ## IC001Q06TA = col_logical(), ## IC001Q07TA = col_logical(), ## IC001Q08TA = col_logical(), ## IC001Q09TA = col_logical(), ## IC001Q10TA = col_logical() ## # ... with 420 more columns ## ) ## See spec(...) for full column specifications. Since we loaded the data, we now want to look at it. We can type its name in the function glimpse() to print some information on the dataset (this code is not run here). glimpse(student_responses) Woah, that’s a big data frame (with a lot of variables with confusing names, to boot)! Great job loading a file and printing it! We are now well on our way to carrying out analysis of our data. 6.7.3 Loading Excel files We will now do the same with an Excel file. You might be thinking that you can open the file in Excel and then save it as a CSV. This is generally a good idea. At the same time, sometimes you may need to directly read a file from Excel. Note that, when possible, we recommend the use of CSV files. They work well across platforms and software (i.e., even if you need to load the file with some other software, such as Python). The package for loading Excel files, readxl, is not a part of the tidyverse, so we will have to install it first (remember, we only need to do this once), and then load it using library(readxl). Note that the command to install readxl is grayed-out below: The # symbol before install.packages(&quot;readxl&quot;) indicates that this line should be treated as a comment and not actually run, like the lines of code that are not grayed-out. It is here just as a reminder that the package needs to be installed if it is not already. Once we have installed readxl, we have to load it (just like tidyverse): install.packages(&quot;readxl&quot;) library(readxl) We can then use the function read_excel() in the same way as read_csv(), where “path/to/file.xlsx” is where an Excel file you want to load is located (note that this code is not run here): my_data &lt;- read_excel(&quot;path/to/file.xlsx&quot;) Of course, were this run, you can replace my_data with a name you like. Generally, it’s best to use short and easy-to-type names for data as you will be typing and using it a lot. Note that one easy way to find the path to a file is to use the “Import Dataset” menu. It is in the Environment window of R Studio. Click on that menu bar option, select the option corresponding to the type of file you are trying to load (e.g., “From Excel”), and then click The “Browse” button beside the File/URL field. Once you click on the, R Studio will automatically generate the file path - and the code to read the file, too - for you. You can copy this code or click Import to load the data. 6.7.4 Loading SAV files The same factors that apply to reading Excel files apply to reading SAV files (from SPSS). NOte that you can also read CSV file directly into SPSS and so because of this and the benefits of using CSVs (they are simple files that work across platforms and software), we recommend using CSVs when possible. First, install the package haven, load it, and the use the function read_sav(): install.packages(&quot;haven&quot;) library(haven) my_data &lt;- read_sav(&quot;path/to/file.xlsx&quot;) 6.7.5 Google Sheets Finally, it can sometimes be useful to load a file directly from Google Sheets, and this can be done using the Google Sheets package. install.packages(&quot;googlesheets&quot;) library(googlesheets) When you run the command below, a link to authenticate with your Google account will open in your browser. my_sheets &lt;- gs_ls() You can then simply use the gs_title() function in conjunction with the gs_read() function: df &lt;- gs_title(&#39;title&#39;) df &lt;- gs_read(df) 6.7.6 Saving Files Using our data frame student_responses, we can save it as a CSV (for example) with the following function. The first argument, student_reponses, is the name of the object that you want to save. The second argument, student-responses.csv, what you want to call the saved dataset. write_csv(student_responses, &quot;student-responses.csv&quot;) That will save a CSV file entitled student-responses.csv in the working directory. If you want to save it to another directory, simply add the file path to the file, i.e. path/to/student-responses.csv. To save a file for SPSS, load the haven package and use write_sav(). There is not a function to save an Excel file, but you can save as a CSV and directly load it in Excel. 6.7.7 Conclusion We will detail the functions used to read every file in a folder (or, to write files to a folder). 6.8 Processing Data Now that we have loaded student_responses into an object, we can process it. This section highlights some common data processing functions. We’re also going to introduce a powerful, unusual operator in R, the pipe. The pipe is this symbol: %&gt;%. It lets you compose functions. It does this by passing the output of one function to the next. A handy shortcut for writing out %&gt;% is Command + Shift + M. Here’s an example. Let’s say that we want to select a few variables from the student_responses dataset and save those variables into a new object, student_mot_vars. Here’s how we would do that using dplyr::select(). student_mot_vars &lt;- # save object student_mot_vars by... student_responses %&gt;% # using dataframe student_responses select(SCIEEFF, JOYSCIE, INTBRSCI, EPIST, INSTSCIE) # and selecting only these five variables Note that we saved the output from the select() function to student_mot_vars but we could also save it back to student_responses, which would simply overwrite the original data frame (the following code is not run here): student_responses &lt;- # save object student_responses by... student_responses %&gt;% # using dataframe student_responses select(student_responses, SCIEEFF, JOYSCIE, INTBRSCI, EPIST, INSTSCIE) # and selecting only these five variables We can also rename the variables at the same time we select them. I put these on separate lines so I could add the comment, but you could do this all in the same line, too. It does not make a difference in terms of how select() will work. student_mot_vars &lt;- # save object student_mot_vars by... student_responses %&gt;% # using dataframe student_responses select(student_efficacy = SCIEEFF, # selecting variable SCIEEFF and renaming to student_efficiency student_joy = JOYSCIE, # selecting variable JOYSCIE and renaming to student_joy student_broad_interest = INTBRSCI, # selecting variable INTBRSCI and renaming to student_broad_interest student_epistemic_beliefs = EPIST, # selecting variable EPIST and renaming to student_epistemic_beliefs student_instrumental_motivation = INSTSCIE # selecting variable INSTSCIE and renaming to student_instrumental_motivation ) [will add more on creating new variables, filtering grouping and summarizing, and joining data sets] 6.9 Communicating / sharing results R Markdown is a highly convenient way to communicate and share results. Navigate to “New File” and then “R Markdown”. [add] Then, click “Knit to PDF”, “Knit to HTML”, or “Knit to Word”. 6.10 Other foundational notes 6.10.1 Configuring R Studio There are a number of changes you can (but do not need to) make to configure R Studio. If you navigate to the Preferences menu in R Studio, you’ll see a number of options you can change, from the appearance of the application to which windows appear where. One important consideration is whether to save your workspace when you close R Studio. By default, R Studio saves all of the objects in your environment. This means that any data that you have loaded–or new data or objects that you have created, such as by merging two data sets together or creating a plot–will, by default, still exist when you open R Studio next. In general, this is not ideal, because it means that you may have taken steps interactively that are not documented your code. This means that when you share your code, or re-run it from the start, it may not work. An easy way to change this is to tell R Studio to start from scratch (in terms of your workspace) each time you open it. You can do that by changing the dropdown menu pointed out in the image below to “Never”. optional caption text While this may seem like a dramatic step - never saving your workspace - it is the foundation for doing reproducible work and research using R Studio (and R). It also represents one of the biggest shifts from using software like Excel or SPSS, where most steps are not documented in code. This involves a shift from thinking that your most permanent and important part of an analysis is your data to thinking of the most important part as being the code: with the code, you can keep your data in its original form, process it, and then save a processed file, through running code. This also means that when you have to make a change to this code, you can re-run the entire analysis easily. 6.10.2 Getting data in and out clipr is a package to easily copy data into and out of R using the clipboard. [add more] datapasta is another option. [add more] "],
["background.html", "Chapter 7 Background 7.1 1. Self-report survey 7.2 2. Log-trace data 7.3 3. Achievement-related and gradebook data 7.4 4. Discussion board data", " Chapter 7 Background In the 2015-2016 and 2016-2017 school years, researchers at Michigan State University carried out a study on students’ motivation to learn in online science classes. The online science classes were part of a statewide online course provider designed to supplement (and not replace) students’ enrollment in their local school. For example, students may choose to enroll in an online physics class because one was not offered at their school (or they were not able to take it given their schedule). The study involved a number of different data sources which were brought to bear to understand students’ motivation: A self-report survey for three distinct but related aspects of students’ motivation Log-trace data, such as data output from the learning management system Achievement-related and gradebook data Discussion board data Achievement-related (i.e., final grade) data First, these different data sources will be described in terms of how they were provided by the school. 7.1 1. Self-report survey This was data collected before the start of the course via self-report survey. The survey included 10 items, each corresponding to one of three measures, namely, for interest, utility value, and perceived competence: I think this course is an interesting subject. (Interest) What I am learning in this class is relevant to my life. (Utility value) I consider this topic to be one of my best subjects. (Perceive competence) I am not interested in this course. (Interest - reverse coded) I think I will like learning about this topic. (Interest) I think what we are studying in this course is useful for me to know. (Utility value) I don’t feel comfortable when it comes to answering questions in this area. (Perceived competence) I think this subject is interesting. (Interest) I find the content of this course to be personally meaningful. (Utility value) I’ve always wanted to learn more about this subject. (Interest) 7.2 2. Log-trace data Log-trace data is data generated from our interactions with digital technologies. In education, an increasingly common source of log-trace data is that generated from interactions with learning management systems. The data for this walk-through is a summary of log-trace data, namely, the number of minutes students spent on the course. Thus, while this data is rich, you can imagine even more complex sources of log-trace data (i.e. timestamps associated with when students started and stopped accessing the course!). 7.3 3. Achievement-related and gradebook data This is a common source of data, namely, one associated with graded assignments students completed. In this walkthrough, we just examine students’ final grade. 7.4 4. Discussion board data Discussion board data is both rich and unstructured, in that it is primarily in the form of written text. We examine a small subset of the discussion board data in this walkthrough. "],
["processing-the-data.html", "Chapter 8 Processing the data 8.1 Viewing the data 8.2 Processing the pre-survey data 8.3 Processing the course data 8.4 Joining the data", " Chapter 8 Processing the data library(readxl) library(tidyverse) library(lubridate) library(here) # Gradebook and log-trace data for F15 and S16 semesters s12_course_data &lt;- read_csv( here( &quot;data&quot;, &quot;online-science-motivation&quot;, &quot;raw&quot;, &quot;s12-course-data.csv&quot; ) ) # Pre-survey for the F15 and S16 semesters s12_pre_survey &lt;- read_csv( here( &quot;data&quot;, &quot;online-science-motivation&quot;, &quot;raw&quot;, &quot;s12-pre-survey.csv&quot; ) ) # Log-trace data for F15 and S16 semesters - ts is for time spent s12_time_spent &lt;- read_csv( here( &quot;data&quot;, &quot;online-science-motivation&quot;, &quot;raw&quot;, &quot;s12-course-minutes.csv&quot; ) ) 8.1 Viewing the data s12_pre_survey ## # A tibble: 1,102 x 16 ## RespondentId StartDate CompletedDate LanguageCode opdata_CourseID ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 426746 2015.08.… &lt;NA&gt; en FrScA-S116-01 ## 2 426775 2015.08.… 2015.08.24 1… en BioA-S116-01 ## 3 427483 2015.08.… &lt;NA&gt; en OcnA-S116-03 ## 4 429883 2015.09.… 2015.09.02 1… en AnPhA-S116-01 ## 5 430158 2015.09.… 2015.09.03 9… en AnPhA-S116-01 ## 6 430161 2015.09.… 2015.09.03 9… en AnPhA-S116-02 ## 7 430162 2015.09.… 2015.09.03 9… en AnPhA-T116-01 ## 8 430167 2015.09.… 2015.09.03 9… en BioA-S116-01 ## 9 430170 2015.09.… 2015.09.03 9… en BioA-T116-01 ## 10 430172 2015.09.… 2015.09.03 9… en PhysA-S116-01 ## # … with 1,092 more rows, and 11 more variables: opdata_username &lt;chr&gt;, ## # Q1MaincellgroupRow1 &lt;dbl&gt;, Q1MaincellgroupRow2 &lt;dbl&gt;, ## # Q1MaincellgroupRow3 &lt;dbl&gt;, Q1MaincellgroupRow4 &lt;dbl&gt;, ## # Q1MaincellgroupRow5 &lt;dbl&gt;, Q1MaincellgroupRow6 &lt;dbl&gt;, ## # Q1MaincellgroupRow7 &lt;dbl&gt;, Q1MaincellgroupRow8 &lt;dbl&gt;, ## # Q1MaincellgroupRow9 &lt;dbl&gt;, Q1MaincellgroupRow10 &lt;dbl&gt; s12_course_data ## # A tibble: 29,711 x 16 ## CourseSectionOr… Bb_UserPK EnrollmentStatus EnrollmentReason Gender ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 2 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 3 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 4 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 5 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 6 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 7 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 8 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 9 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 10 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## # … with 29,701 more rows, and 11 more variables: FinalGradeCEMS &lt;dbl&gt;, ## # Gradebook_Item &lt;chr&gt;, Item_Position &lt;dbl&gt;, Gradebook_Type &lt;chr&gt;, ## # Gradebook_Date &lt;chr&gt;, Grade_Category &lt;chr&gt;, Status &lt;lgl&gt;, ## # Points_Earned &lt;chr&gt;, Points_Attempted &lt;dbl&gt;, Points_Possible &lt;dbl&gt;, ## # last_access_date &lt;drtn&gt; s12_time_spent ## # A tibble: 598 x 6 ## CourseID CourseSectionID CourseSectionOrigID Bb_UserPK CUPK TimeSpent ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 27 17146 OcnA-S116-01 44638 190682 1383. ## 2 27 17146 OcnA-S116-01 54346 194259 1191. ## 3 27 17146 OcnA-S116-01 57981 196014 3343. ## 4 27 17146 OcnA-S116-01 66740 190463 965. ## 5 27 17146 OcnA-S116-01 67920 191593 4095. ## 6 27 17146 OcnA-S116-01 85355 190104 595. ## 7 27 17146 OcnA-S116-01 85644 190685 1632. ## 8 27 17146 OcnA-S116-01 86349 191713 1601. ## 9 27 17146 OcnA-S116-01 86460 191887 1891. ## 10 27 17146 OcnA-S116-01 87970 194256 3123. ## # … with 588 more rows 8.2 Processing the pre-survey data Often, survey data needs to be processed in order to be (most) useful. Here, we process the self-report items into three scales, for: interest, self-efficacy, and utility value. We do this by Renaming the question variables to something more managable Reversing the response scales on questions 4 and 7 Categorizing each question into a measure Computing the mean of each measure s12_pre_survey &lt;- s12_pre_survey %&gt;% # Rename the qustions something easier to work with because R is case sensitive # and working with variable names in mix case is prone to error rename(q1 = Q1MaincellgroupRow1, q2 = Q1MaincellgroupRow2, q3 = Q1MaincellgroupRow3, q4 = Q1MaincellgroupRow4, q5 = Q1MaincellgroupRow5, q6 = Q1MaincellgroupRow6, q7 = Q1MaincellgroupRow7, q8 = Q1MaincellgroupRow8, q9 = Q1MaincellgroupRow9, q10 = Q1MaincellgroupRow10) %&gt;% # Convert all question responses to numeric mutate_at(vars(q1:q10), funs(as.numeric)) # Function for reversing scales reverse_scale &lt;- function(question) { # Reverses the response scales for consistency # Args: # question: survey question # Returns: a numeric converted response # Note: even though 3 is not transformed, case_when expects a match for all # possible conditions, so it&#39;s best practice to label each possible input # and use TRUE ~ as the final statement returning NA for unexpected inputs x &lt;- case_when(question == 1 ~ 5, question == 2 ~ 4, question == 4 ~ 2, question == 5 ~ 1, question == 3 ~ 3, TRUE ~ NA_real_) x } # Reverse scale for questions 4 and 7 s12_pre_survey &lt;- s12_pre_survey %&gt;% mutate(q4 = reverse_scale(q4), q7 = reverse_scale(q7)) # Add measure variable s12_measure_mean &lt;- s12_pre_survey %&gt;% # Gather questions and responses gather(question, response, c(q1:q10)) %&gt;% mutate( measure = case_when( question %in% c(&quot;q1&quot;, &quot;q4&quot;, &quot;q5&quot;, &quot;q8&quot;, &quot;q10&quot;) ~ &quot;int&quot;, question %in% c(&quot;q2&quot;, &quot;q6&quot;, &quot;q9&quot;) ~ &quot;uv&quot;, question %in% c(&quot;q3&quot;, &quot;q7&quot;) ~ &quot;pc&quot;, TRUE ~ NA_character_ )) %&gt;% group_by(measure) %&gt;% summarise( # Mean response for each measure mean_response = mean(response, na.rm = TRUE), # Percent of each measure that had NAs in the response field percent_NA = mean(is.na(response)) ) s12_measure_mean ## # A tibble: 3 x 3 ## measure mean_response percent_NA ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 int 4.26 0.171 ## 2 pc 3.65 0.170 ## 3 uv 3.76 0.170 8.3 Processing the course data We also can process the course data in order to obtain more information. # split course section into components s12_course_data &lt;- s12_course_data %&gt;% separate(col = CourseSectionOrigId, into = c(&#39;subject&#39;, &#39;semester&#39;, &#39;section&#39;), sep = &#39;-&#39;, remove = FALSE) ## Error in eval_tidy(enquo(var), var_env): object &#39;CourseSectionOrigId&#39; not found This led to pulling out the subject, semester, and section from the course ID; variables that we can use later on. 8.4 Joining the data To join the course data and pre-survey data, we need to create similar keys. In other words, our goal here is to have one variable that matches across both datasets, so that we can merge the datasets on the basis of that variable. For these data, both have variables for the course and the student, though they have different names in each. Our first goal will be to rename two variables in each of our datasets so that they will match. One variable will correspond to the course, and the other will correspond to the student. We are not changing anything in the data itself at this step - instead, we are just cleaning it up so that we can look at the data all in one place. Let’s start with the pre-survey data. We will rename RespondentID and opdata_CourseID to be student_id and course_id, respectively. s12_pre_survey &lt;- s12_pre_survey %&gt;% rename(student_id = RespondentId, course_id = opdata_CourseID) s12_pre_survey ## # A tibble: 1,102 x 16 ## student_id StartDate CompletedDate LanguageCode course_id ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 426746 2015.08.… &lt;NA&gt; en FrScA-S1… ## 2 426775 2015.08.… 2015.08.24 1… en BioA-S11… ## 3 427483 2015.08.… &lt;NA&gt; en OcnA-S11… ## 4 429883 2015.09.… 2015.09.02 1… en AnPhA-S1… ## 5 430158 2015.09.… 2015.09.03 9… en AnPhA-S1… ## 6 430161 2015.09.… 2015.09.03 9… en AnPhA-S1… ## 7 430162 2015.09.… 2015.09.03 9… en AnPhA-T1… ## 8 430167 2015.09.… 2015.09.03 9… en BioA-S11… ## 9 430170 2015.09.… 2015.09.03 9… en BioA-T11… ## 10 430172 2015.09.… 2015.09.03 9… en PhysA-S1… ## # … with 1,092 more rows, and 11 more variables: opdata_username &lt;chr&gt;, ## # q1 &lt;dbl&gt;, q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;, q7 &lt;dbl&gt;, ## # q8 &lt;dbl&gt;, q9 &lt;dbl&gt;, q10 &lt;dbl&gt; Looks better now! Let’s proceed to the course data. Our goal is to rename two variables that correspond to the course and the student so that we can match with the other variables we just created for the pre-survey data. s12_course_data &lt;- s12_course_data %&gt;% rename(student_id = Bb_UserPK, course_id = CourseSectionOrigID) s12_course_data ## # A tibble: 29,711 x 16 ## course_id student_id EnrollmentStatus EnrollmentReason Gender ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 2 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 3 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 4 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 5 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 6 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 7 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 8 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 9 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 10 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## # … with 29,701 more rows, and 11 more variables: FinalGradeCEMS &lt;dbl&gt;, ## # Gradebook_Item &lt;chr&gt;, Item_Position &lt;dbl&gt;, Gradebook_Type &lt;chr&gt;, ## # Gradebook_Date &lt;chr&gt;, Grade_Category &lt;chr&gt;, Status &lt;lgl&gt;, ## # Points_Earned &lt;chr&gt;, Points_Attempted &lt;dbl&gt;, Points_Possible &lt;dbl&gt;, ## # last_access_date &lt;drtn&gt; Now that we have two variables that are consistent across both datasets - we have called them “course_id” and “student_id” - we can join these using the dplyr function, left_join(). Let’s save our joined data as a new object called “dat.” dat &lt;- left_join(s12_course_data, s12_pre_survey, by = c(&quot;student_id&quot;, &quot;course_id&quot;)) dat ## # A tibble: 29,711 x 30 ## course_id student_id EnrollmentStatus EnrollmentReason Gender ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 2 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 3 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 4 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 5 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 6 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 7 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 8 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 9 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 10 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## # … with 29,701 more rows, and 25 more variables: FinalGradeCEMS &lt;dbl&gt;, ## # Gradebook_Item &lt;chr&gt;, Item_Position &lt;dbl&gt;, Gradebook_Type &lt;chr&gt;, ## # Gradebook_Date &lt;chr&gt;, Grade_Category &lt;chr&gt;, Status &lt;lgl&gt;, ## # Points_Earned &lt;chr&gt;, Points_Attempted &lt;dbl&gt;, Points_Possible &lt;dbl&gt;, ## # last_access_date &lt;drtn&gt;, StartDate &lt;chr&gt;, CompletedDate &lt;chr&gt;, ## # LanguageCode &lt;chr&gt;, opdata_username &lt;chr&gt;, q1 &lt;dbl&gt;, q2 &lt;dbl&gt;, ## # q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;, q7 &lt;dbl&gt;, q8 &lt;dbl&gt;, q9 &lt;dbl&gt;, ## # q10 &lt;dbl&gt; Just one more data frame to merge: s12_time_spent &lt;- s12_time_spent %&gt;% rename(student_id = Bb_UserPK, course_id = CourseSectionOrigID) s12_time_spent &lt;- s12_time_spent %&gt;% mutate(student_id = as.integer(student_id)) dat &lt;- dat %&gt;% left_join(s12_time_spent, by = c(&quot;student_id&quot;, &quot;course_id&quot;)) Note that they’re now combined, even though the course data has many more rows: The pre_survey data has been joined for each student by course combination. We have a pretty large data frame! Let’s take a quick look. dat ## # A tibble: 29,711 x 34 ## course_id student_id EnrollmentStatus EnrollmentReason Gender ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 2 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 3 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 4 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 5 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 6 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 7 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 8 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 9 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## 10 AnPhA-S1… 60186 Approved/Enroll… Course Unavaila… M ## # … with 29,701 more rows, and 29 more variables: FinalGradeCEMS &lt;dbl&gt;, ## # Gradebook_Item &lt;chr&gt;, Item_Position &lt;dbl&gt;, Gradebook_Type &lt;chr&gt;, ## # Gradebook_Date &lt;chr&gt;, Grade_Category &lt;chr&gt;, Status &lt;lgl&gt;, ## # Points_Earned &lt;chr&gt;, Points_Attempted &lt;dbl&gt;, Points_Possible &lt;dbl&gt;, ## # last_access_date &lt;drtn&gt;, StartDate &lt;chr&gt;, CompletedDate &lt;chr&gt;, ## # LanguageCode &lt;chr&gt;, opdata_username &lt;chr&gt;, q1 &lt;dbl&gt;, q2 &lt;dbl&gt;, ## # q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;, q7 &lt;dbl&gt;, q8 &lt;dbl&gt;, q9 &lt;dbl&gt;, ## # q10 &lt;dbl&gt;, CourseID &lt;dbl&gt;, CourseSectionID &lt;dbl&gt;, CUPK &lt;dbl&gt;, ## # TimeSpent &lt;dbl&gt; It looks like we have neary 30,000 observations from 30 variables. Now that our data are ready to go, we can start to ask some questions of the data. "],
["visualizations-and-models.html", "Chapter 9 Visualizations and Models 9.1 Visualization of the relationship between time spent on course and percentage of points earned", " Chapter 9 Visualizations and Models One thing we might be wondering is how time spent on course is related to students’ final grade. Let’s first calculate the percentage of points students earned as a measure of their final grade (noting that the teacher may have assigned a different grade–or weighted their grades in ways not reflected through the points). dat &lt;- dat %&gt;% group_by(student_id, course_id) %&gt;% mutate(Points_Earned = as.integer(Points_Earned)) %&gt;% summarize(total_points_possible = sum(Points_Possible, na.rm = TRUE), total_points_earned = sum(Points_Earned, na.rm = TRUE)) %&gt;% mutate(percentage_earned = total_points_earned/total_points_possible) %&gt;% ungroup() %&gt;% left_join(dat) # note that we join this back to the original data frame to retain all of the variables 9.1 Visualization of the relationship between time spent on course and percentage of points earned ggplot(dat, aes(x = TimeSpent, y = percentage_earned)) + geom_point() There appears to be some relationship. What if we added a line of best fit - a linear model? ggplot(dat, aes(x = TimeSpent, y = percentage_earned)) + geom_point() + geom_smooth(method = &quot;lm&quot;) So, it appeares that the more time students spent on the course, the more points they earned. "],
["linear-model-regression.html", "Chapter 10 Linear model (regression) 10.1 But what about different courses? 10.2 Introduction 10.3 Driving Question and Objectives 10.4 Data Import 10.5 R Markdown 10.6 Including Plots", " Chapter 10 Linear model (regression) We can find out exactly what the relationship is using a linear model. m_linear &lt;- lm(percentage_earned ~ TimeSpent, data = dat) summary(m_linear) ## ## Call: ## lm(formula = percentage_earned ~ TimeSpent, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.63001 -0.07894 0.05366 0.15742 0.34544 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.102e-01 2.158e-03 282.77 &lt;2e-16 *** ## TimeSpent 7.983e-05 9.399e-07 84.94 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2236 on 29481 degrees of freedom ## (228 observations deleted due to missingness) ## Multiple R-squared: 0.1966, Adjusted R-squared: 0.1966 ## F-statistic: 7214 on 1 and 29481 DF, p-value: &lt; 2.2e-16 10.1 But what about different courses? Is there course-specific differences in how much time students spend on the course as well as in how time spent is related to the percentage of points students earned? ggplot(dat, aes(x = TimeSpent, y = percentage_earned, color = course_id)) + geom_point() ggplot(dat, aes(x = TimeSpent, y = percentage_earned, color = course_id)) + geom_point() + geom_smooth(method = &quot;lm&quot;) There appears to be so. One way we can test is to use what is called a multi-level model. This requires a new package; one of the most common for estimating these types of models is lme4. We use it very similarly to the lm() function, but we pass it an additional argument about what the groups, or levels, in the data are. # install.packages(&quot;lme4&quot;) library(lme4) m_course &lt;- lmer(percentage_earned ~ TimeSpent + (1|course_id), data = dat) summary(m_course) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: percentage_earned ~ TimeSpent + (1 | course_id) ## Data: dat ## ## REML criterion at convergence: -8619.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.3371 -0.3701 0.2154 0.6418 2.1217 ## ## Random effects: ## Groups Name Variance Std.Dev. ## course_id (Intercept) 0.008815 0.09389 ## Residual 0.043475 0.20851 ## Number of obs: 29483, groups: course_id, 26 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 5.820e-01 1.863e-02 31.25 ## TimeSpent 8.884e-05 9.273e-07 95.81 ## ## Correlation of Fixed Effects: ## (Intr) ## TimeSpent -0.090 ## fit warnings: ## Some predictor variables are on very different scales: consider rescaling A common way to understand how much variability is at the group level is to calculate the intra-class correlation. This value is the proportion of the variability in the outcome (the y-variable) that is accounted for solely by the groups identified in the model. There is a useful function in the sjstats package for doing this. # install.packages(&quot;sjstats&quot;) library(sjstats) icc(m_course) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.169 ## Conditional ICC: 0.131 This shows that nearly 17% of the variability in the percentage of points students earned can be explained simply by knowing what class they are in. 10.2 Introduction Gradebooks are nearly ubiquitous throughout K-12 classrooms, whether they exist as standalone Excel files, Google Sheets, or in proprietary software. This walkthrough goes through a series of analyses using the data science framework (link), using the sample Assessment Types - Points Excel gradebook template from MIT. All data in the sample gradebook have been generated, and do not reflect individual student data. 10.3 Driving Question and Objectives 10.4 Data Import Setting up our environment (note: how deep do we go into working directories?!) Importing our data (need to sim data for 25 students) Check text for object naming conventions, discussion of .csv, .xlsx, versatility of import functions within the tidyverse File naming - issues that can arise from spaces gradebook &lt;- readxl::read_excel(here(&quot;/data/gradebooks&quot;, &quot;ExcelGradeTrackerAssessmentTypePoints_SIMDATA_01.xlsx&quot;)) ## New names: ## * `` -&gt; ...1 ## * `` -&gt; ...3 ## * `` -&gt; ...4 ## * `` -&gt; ...5 ## * `` -&gt; ...6 ## * … and 39 more problems 10.5 R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 10.6 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. # Seed for random number generation set.seed(42) knitr::opts_chunk$set(cache.extra = knitr::rand_seed, eval = TRUE, echo = FALSE, results = &#39;hide&#39;, message = FALSE, warning = FALSE) "],
["background-1.html", "Chapter 11 Background", " Chapter 11 Background One area of interest is the delivery of online instruction, which is becoming more prevalent: in 2007, over 3.9 million U.S. students were enrolled one or more online courses (Allen &amp; Seaman, 2008). In this walkthrough, we examine the educational experiences of students in online science courses at a virtual middle school in order to characterize their motivation to achieve and their tangible engagement with the course in terms of behavioral trace measures. To do so, we use a robust data set, which includes self-reported motivation as well as behavioral trace data collected from a learning management system (LMS) to identify predictors of final course grade. Our work examines the idea of educational success in terms of student interactions with an online science course. One meaningful perspective from which to consider students’ engagement with online courses is related to their motivation to achieve. More specifically, it is important to consider how and why students are engaging with the course. Considering the psychological mechanisms behind achievement is valuable because doing so may help to identify meaningful points of intervention for educators and for researchers and administrators in online and face-to-face courses interested in the intersection between behavioral trace measures and students’ motivational and affective experiences in such courses. We take up the following four questions: Is motivation more predictive of course grades as compared to other online indicators of engagement? Which types of motivation are most predictive of achievement? Which types of trace measures are most predictive of achievement? How does a random forest compare to a simple linear model (regression)? "],
["information-about-the-dataset.html", "Chapter 12 Information about the dataset", " Chapter 12 Information about the dataset This dataset came from 499 students enrolled in online middle school science courses in 2015-2016. The data were originally collected for use as a part of a research study, though the findings have not been published anywhere, yet. The setting of this study was a public, provider of individual online courses in a Midwestern state. In particular, the context was two semesters (Fall and Spring) of offerings of five online science courses (Anatomy &amp; Physiology, Forensic Science, Oceanography, Physics, and Biology), with a total of 36 classes. Specific information in the dataset included: a pre-course survey students completed about their self-reported motivation in science — in particular, their perceived competence, utility value, and interest the time students spent on the course (obtained from the LMS, Blackboard) and their final course grades as well as their involvement in discussion forums 12 for discussion board responses, we used the Linguistic Inquiry and Word Count (LIWC; Pennebaker, Boyd, Jordan, &amp; Blackburn, 2015) to calculate the number of posts per student and variables for the mean levels of students’ cognitive processing, positive affect, negative affect, and social-related discourse title: ‘Education Dataset Analysis Pipeline: Walk Through #3’ output: html_document — "],
["background-2.html", "Chapter 13 Background", " Chapter 13 Background One area of interest is the delivery of online instruction, which is becoming more prevalent: in 2007, over 3.9 million U.S. students were enrolled one or more online courses (Allen &amp; Seaman, 2008). In this walkthrough, we examine the educational experiences of students in online science courses at a virtual middle school in order to characterize their motivation to achieve and their tangible engagement with the course in terms of behavioral trace measures. To do so, we use a robust data set, which includes self-reported motivation as well as behavioral trace data collected from a learning management system (LMS) to identify predictors of final course grade. Our work examines the idea of educational success in terms of student interactions with an online science course. One meaningful perspective from which to consider students’ engagement with online courses is related to their motivation to achieve. More specifically, it is important to consider how and why students are engaging with the course. Considering the psychological mechanisms behind achievement is valuable because doing so may help to identify meaningful points of intervention for educators and for researchers and administrators in online and face-to-face courses interested in the intersection between behavioral trace measures and students’ motivational and affective experiences in such courses. We take up the following four questions: Is motivation more predictive of course grades as compared to other online indicators of engagement? Which types of motivation are most predictive of achievement? Which types of trace measures are most predictive of achievement? How does a random forest compare to a simple linear model (regression)? "],
["information-about-the-dataset-1.html", "Chapter 14 Information about the dataset", " Chapter 14 Information about the dataset This dataset came from 499 students enrolled in online middle school science courses in 2015-2016. The data were originally collected for use as a part of a research study, though the findings have not been published anywhere, yet. The setting of this study was a public, provider of individual online courses in a Midwestern state. In particular, the context was two semesters (Fall and Spring) of offerings of five online science courses (Anatomy &amp; Physiology, Forensic Science, Oceanography, Physics, and Biology), with a total of 36 classes. Specific information in the dataset included: a pre-course survey students completed about their self-reported motivation in science — in particular, their perceived competence, utility value, and interest the time students spent on the course (obtained from the LMS, Blackboard) and their final course grades as well as their involvement in discussion forums for discussion board responses, we used the Linguistic Inquiry and Word Count (LIWC; Pennebaker, Boyd, Jordan, &amp; Blackburn, 2015) to calculate the number of posts per student and variables for the mean levels of students’ cognitive processing, positive affect, negative affect, and social-related discourse At the beginning of the semester, students were asked to complete the pre-course survey about their perceived competence, utility value, and interest. At the end of the semester, the time students spent on the course, their final course grades, and the contents of the discussion forums were collected. In this walkthrough, we used the R package caret to carry out the analyses. "],
["information-on-random-forests.html", "Chapter 15 Information on random forests", " Chapter 15 Information on random forests 500 trees were grown as part of our random forest. We partitioned the data before conducting the main analysis so that neither the training nor the testing data set would be disproportionately representative of high-achieving or low-achieving students. The training data set consisted of 80% of the original data (n = 400 cases), whereas the testing data set consisted of 20% of the original data (n = 99 cases). We built our random forest model on the training data set, and then evaluated the model on the testing data set. Three variables were tried at each node. Note that the random forest algorithm does not accept cases with missing data, and so we deleted cases listwise if data were missing. This decision eliminated 51 cases from our original data set, to bring us to our final sample size of 499 unique students. For our analyses, we used Random Forest modeling (Breiman, 2001). Random forest is an extension of decision tree modeling, whereby a collection of decision trees are simultaneously “grown” and are evaluated based on out-of-sample predictive accuracy (Breiman, 2001). Random forest is random in two main ways: first, each tree is only allowed to “see” and split on a limited number of predictors instead of all the predictors in the parameter space; second, a random subsample of the data is used to grow each individual tree, such that no individual case is weighted too heavily in the final prediction. Whereas some machine learning approaches (e.g., boosted trees) would utilize an iterative model-building approach, random forest estimates all the decision trees at once. In this way, each tree is independent of every other tree. Thus, the random forest algorithm provides a robust regression approach that is distinct from other modeling approaches. The final random forest model aggregates the findings across all the separate trees in the forest in order to offer a collection of “most important” variables as well as a percent variance explained for the final model. A random forest is well suited to the research questions that we had here because it allows for nonlinear modeling. We hypothesized complex relationships between students’ motivation, their engagement with the online courses, and their achievement. For this reason, a traditional regressive or structural equation model would have been insufficient to model the parameter space we were interesting in modeling. Our random forest model had one outcome and eleven predictors. A common tuning parameter for machine learning models is the number of variables considered at each split (Kuhn, 2008); we considered three variables at each split for this analysis. The outcome was the final course grade that the student earned. The predictor variables included motivation variables (interest value, utility value, and science perceived competence) and trace variables (the amount of time spent in the course, the course name, the number of discussion board posts over the course of the semester, the mean level of cognitive processing evident in discussion board posts, the positive affect evident in discussion board posts, the negative affect evident in discussion board posts, and the social-related discourse evident in their discussion board posts). We used this random forest model to address all three of our research questions. To interpret our findings, we examined three main things: (1) predictive accuracy of the random forest model, (2) variable importance, and (3) variance explained by the final random forest model. "],
["analysis.html", "Chapter 16 Analysis 16.1 Use of caret 16.2 Predicted values", " Chapter 16 Analysis First, we will load the data, filter the data to include only the data from one year, and select variables of interest. 16.1 Use of caret Here, we remove observations with missing data (per our note above about random forests requiring complete cases). First, machine learning methods often involve using a large number of variables. Oftentimes, some of these variables will not be suitable to use: they may be highly correlated with other variables, for instance, or may have very little - or no - variability. Indeed, for the data set used in this study, one variables has the same (character string) value for all of the observations. We can detect this variable and any others using the following function: If we look at enrollment_status, we will see that it is “Approved/Enrolled” for all of the students. When we use this in certian models, t may cause some problems, and so we remove it first. Note that many times you may wish to pre-process the variables, such as by centering or scaling them; we could this with code like the following, which is not run here, as we will first try this out with the variables’ original values. We will want to make character string variables into factors. Now, we will prepare the train and test datasets, using the caret function for creating data partitions. Here, the p argument specifies what proportion of the data we want to be in the training partition. Note that this function splits the data based upon the outcome, so that the training and test data sets will both have comparable values for the outcome. Note the times = 1 argument; this function can be used to create multiple train and test sets, something we will describe in more detail later. Finally, we will estimate the models. Here, we will use the train function, passing all of the variables in the data frame (except for the outcome, or dependent variable, final_grade) as predictors. NOte that you can read more about the specific random forest implementation chosen here. ## Error: Required package is missing ## Error in eval(expr, envir, enclos): object &#39;rf_fit&#39; not found We have some results! First, we see that we have 400 samples, or 400 observations, the number in the train data set. No pre-processing steps were specified in the model fitting (note that) these the output of preProcess can be passed to train() to center, scale, and transform the data in many other ways. Next, note that a resampling technique has been used: this is not for validating the model (per se), but is rather for selecting tuning parameters, or options that need to be specified as a part of the modeling. These parameters can be manually provided, or can be estimated via strategies such as the bootstrap resample (or k-folds cross validation). It appears that the model with the value of the mtry tuning parameter equal to 42 seemed to explain the data best, the splirule* being “extratrees”, and min.node.size** held constant at a value of 5. Let’s see if we end up with slightly different values if we change the resampling technique to cross-validation, instead of bootstrap resampling. ## Error: Required package is missing ## Error in eval(expr, envir, enclos): object &#39;rf_fit1&#39; not found The same tuning parameter values seem to be found with this method. Let’s check just one last thing - what if we do not fix min.node.size to five? Let’s create our own grid of values to test. We’ll stick with the default bootstrap resampling method to choose the best model. ## Error: Required package is missing ## Error in eval(expr, envir, enclos): object &#39;rf_fit2&#39; not found The model with the same values as identified before but with min.node.size equal to 1 seems to fit best, though the improvement seems to be fairly small relative to the difference the other tuning parameters seem to make. Let’s take a look at this model. We will first note the large number of independent variables: this is due to the factors being treated as dummy codes. We can also note the *OOB prediction error (MSE)`, of 0.351, and the proportion of the variance explained, or R squared, of 0.658. ## Error in eval(expr, envir, enclos): object &#39;rf_fit2&#39; not found 16.2 Predicted values Using the simpler model without treating min.node.size as a tuning paramete In particular, let’s explore predicted values. In particular, we can see how predicted values compare to those in the test set to understand predictive accuracy. First, let’s calculte the Root Mean Square Error (RMSE), just to gain practice working with the model output. ## Error in predict(rf_fit, d_train): object &#39;rf_fit&#39; not found ## Error in eval(lhs, parent, parent): object &#39;d_train_augmented&#39; not found We can calculate these automatically using the caret defaultSummary() function (which just requires columns with obs and pred in it in a data frame): ## Error in `[.data.frame`(data, , &quot;pred&quot;): undefined columns selected The RMSE and MAE values correspond to those we calculated manually. Note that an \\(R^2\\) value is also calculated as the square of the correlation between the observed and predicted values. So, what do these values tell us? On average, our predictions are around 4 percentage points away from their actual values. Not so bad! Why use RMSE, though, over MAE? [Note: don’t kow why]. "],
["examining-predictive-accuracy-on-the-test-data-set.html", "Chapter 17 Examining predictive accuracy on the test data set 17.1 Variable importance measures 17.2 Comparing a random forest to a regression 17.3 Background 17.4 Creating a network graph 17.5 Clustering 17.6 Network Visualization with Clusters", " Chapter 17 Examining predictive accuracy on the test data set What if we use the test data set - data not used to train the model? We can compare this to the values above to see how much poorer - if at all - performance is on data not used to train the model. 17.1 Variable importance measures We can examine two different variable importance measures using the ranger method in caret. Note that importance values are not calcultaed automatically, but that “impurity” or “permutation” can be passed to the importance argument in train(). See more here. We’ll re-run the model, but will add an argument to call the variable importance metric. ## Error: Required package is missing ## Error in varImp(rf_fit_imp): object &#39;rf_fit_imp&#39; not found We can visualize these: ## Error in varImp(rf_fit_imp): object &#39;rf_fit_imp&#39; not found We can see whether these change with the different importance measures. ## Error: Required package is missing ## Error in varImp(rf_fit_imp_permutation): object &#39;rf_fit_imp_permutation&#39; not found They are similar but somewhat different. One takeaway from this analysis is that what course students are in seems to have a different effect depending on the course. Also, how much students write in their discussion posts (n) seems to be very important - as does the time students spend in their course. Finally, there are some subject level differences (in terms of how predictive subject is). Perhaps grades should be normalized within subject: would this still be an important predictor, then? 17.2 Comparing a random forest to a regression You may be curious about comparing the predictive accuracy of the model to a linear model (a regression). ## Error in as.data.frame(d_train_augmented): object &#39;d_train_augmented&#39; not found We can see that the random forest technique seems to perform better than regression. It may be interesting to compare the results from the random forest not to a more straightforward model, such as a regression, but to a more sophisticated model, like one for deep learning. For now, we’ll leave that to you. 17.3 Background Relationships are important to us. In the case of many research techniques, relationships are—deservedly—the focus of analyses. It is not very difficult to imagine qualitative techniques to study relationships: one could ask other individuals about who their friends are, why they are their friends, and what they like to do when with them. Increasingly, it is also not hard to imagine quantitative techniques to study relationships, too. In a way, the same questions that could be used qualitatively can serve as the basis for the quantitative study of relationships. Indeed, social network analysis uses these relations in a range of visualizations as well as statistical models. 17.4 Creating a network graph Now, we’ll use the igraph package to create a graph of our simulated data’s network. Once we have this graph, we can calculate many useful descriptive statistics associated with the network’s features: Diameter: The length of the longest geodesic; that is, the max distance between two nodes in the network. Density: The ratio of the number of edges and the number of possible edges for a network of that size. Transitivity: The balance of connections. Also called the clustering coefficient. The probability that the adjacent vertices of a vertex are connected. When the clustering coefficient is large it implies that a graph is highly clustered around a few nodes; when it is low it implies that the links in the graph are relatively evenly spread among all the nodes (Hogan, 2017). Reciprocity: The proportion of mutual connections (in a directed network). The probability that the opposite counterpart of a directed edge is also included in the graph. Degree: The number of connections someone has with others nominating or being a nominee (Kadushin, 2012). Keep in mind that measures such as diameter and density “can be misleading when comparing graphs of substantially different sizes” (Hogan, 2017, p. 255). Therefore, these measures should only be used to compare networks of similar size, or the same network at different points in time. First, let’s create the network graph, called sim_graph: library(igraph) ## Error in library(igraph): there is no package called &#39;igraph&#39; sim_graph &lt;- data %&gt;% select(nominator, nominee) %&gt;% # this creates an edgelist as.matrix %&gt;% graph_from_edgelist(directed=TRUE) %&gt;% set_vertex_attr(name=&#39;degree&#39;, value=degree(., mode=&#39;total&#39;, loops=FALSE)) %&gt;% set_vertex_attr(name=&#39;in_degree&#39;, value=degree(., mode=&#39;in&#39;, loops=FALSE)) %&gt;% set_vertex_attr(name=&#39;out_degree&#39;, value=degree(., mode=&#39;out&#39;, loops=FALSE)) ## Error in data %&gt;% select(nominator, nominee) %&gt;% as.matrix %&gt;% graph_from_edgelist(directed = TRUE) %&gt;% : could not find function &quot;%&gt;%&quot; network_summary &lt;- sim_graph %&gt;% V %&gt;% length %&gt;% # number of vertices/nodes rbind(sim_graph %&gt;% gsize) %&gt;% # number of edges rbind(sim_graph %&gt;% diameter) %&gt;% # max distance between two vertices rbind({sim_graph %&gt;% edge_density * 100} %&gt;% round(2)) %&gt;% rbind({sim_graph %&gt;% transitivity(&quot;global&quot;) * 100} %&gt;% round(2)) %&gt;% rbind({sim_graph %&gt;% reciprocity * 100} %&gt;% round(2)) %&gt;% rbind(sim_graph %&gt;% vertex_attr(&#39;degree&#39;) %&gt;% mean %&gt;% round(2)) %&gt;% rbind(sim_graph %&gt;% vertex_attr(&#39;degree&#39;) %&gt;% sd %&gt;% round(2)) %&gt;% rbind(sim_graph %&gt;% vertex_attr(&#39;degree&#39;) %&gt;% median) %&gt;% rbind(sim_graph %&gt;% vertex_attr(&#39;degree&#39;) %&gt;% min) %&gt;% rbind(sim_graph %&gt;% vertex_attr(&#39;degree&#39;) %&gt;% max) ## Error in sim_graph %&gt;% V %&gt;% length %&gt;% rbind(sim_graph %&gt;% gsize) %&gt;% : could not find function &quot;%&gt;%&quot; colnames(network_summary) &lt;- c(&quot;&quot;) ## Error in colnames(network_summary) &lt;- c(&quot;&quot;): object &#39;network_summary&#39; not found rownames(network_summary) &lt;- c(&quot;Number of nodes: &quot;, &quot;Number of edges: &quot;, &quot;Diameter: &quot;, &quot;Density: &quot;, &quot;Transitivity: &quot;, &quot;Reciprocity: &quot;, &quot;Mean degree: &quot;, &quot;SD degree: &quot;, &quot;Median degree: &quot;, &quot;Min degree: &quot;, &quot;Max degree: &quot;) ## Error in rownames(network_summary) &lt;- c(&quot;Number of nodes: &quot;, &quot;Number of edges: &quot;, : object &#39;network_summary&#39; not found network_summary ## Error in eval(expr, envir, enclos): object &#39;network_summary&#39; not found 17.5 Clustering With a graph of our simulated network, we can see if clustering occurs in this network and describe some characteristics of these clusters. First, a definition: a cluster—also called a community or group—is a set of nodes with many edges inside the community and few edges between outside it (i.e. between the community itself and the rest of the graph). There are numerous methods for determining network clusters, but here we use the spinglass clustering algorithm, which maps community detection onto finding the ground state of an infinite range spin glass. Csardi, Nepusz, and Airoldi (2016, pp. 132-133) explained: The clustering method of Reichardt and Bornholdt (2006) is motivated by spin glass models from statistical physics. Such models are used to describe and explain magnetism at the microscopic scale at finite temperatures. Reichardt and Bornholdt (2006) drew an analogy between spin glass models and the problem of community detection on graphs and proposed an algorithm based on the simulated annealing of the spin glass model to obtain well-defined communities in a graph. A spin glass model consists of a set of particles called spins that are coupled by ferromagnetic or antiferromagnetic bonds. Each spin can be in one of k possible states. The well-known Potts model then defines the total energy of the spin glass with a given spin configuration… Spins and interactions in the Potts model are very similar to graphs: each spin in the model corresponds to a vertex, and each interaction corresponds to an edge… Reichardt and Bornholdt (2006) gave efficient update rules for the above energy function, making it possible to apply a simulated annealing procedure to find the ground state of the model that corresponds to a low energy configuration. Their algorithm starts from a random configuration of spins and tries to flip all the spins once in each time step. After each individual spin flip, the energy of the new configuration is evaluated. In other words, the spinglass clustering algorithm partitions nodes into communities by optimizing an energy function. The energy is optimized using the following function (Reichardt and Bornholdt, 2008): \\[H({\\sigma}) = -\\sum(a_{ij} \\textrm{internal links}) + \\sum(b_{ij}\\textrm{internal non-links}) + \\sum(c_{ij}\\textrm{external links}) - \\sum(d_{ij}\\textrm{external non-links})\\]. This function penalizes missing edges or non-links of people/nodes in the same cluster and present links or edges between people/nodes in different clusters. It also rewards present links or edges between people/nodes in the same cluster and missing links or edges between people/nodes in different clusters. Thus, a lower score (i.e., lower energy level) is better as it means that the internal links and external non-links have more weightage in that model. In other words, in a strong model, members within clusters are strongly linked and members in separate clusters are weakly linked . Here, \\(a_{ij}, b_{ij}, c_{ij}, d_{ij}\\) represent the individual weights of the four components. The initial R code to produce spinglass clusters is straightforward. First, we identify one “giant” cluster—basically, all nodes that have even a loose connection to each other and create a new network graph by removing any nodes that are not part of the giant cluster. Let’s call the graph of the giant cluster giant_cluster: library(igraph) ## Error in library(igraph): there is no package called &#39;igraph&#39; giant_cluster &lt;- sim_graph %&gt;% set_vertex_attr(name=&#39;membership&#39;, value = clusters(sim_graph) %&gt;% as.data.frame %&gt;% pull(membership) ) %&gt;% delete_vertices({vertex_attr(., &#39;membership&#39;) != 1} %&gt;% which) ## Error in sim_graph %&gt;% set_vertex_attr(name = &quot;membership&quot;, value = clusters(sim_graph) %&gt;% : could not find function &quot;%&gt;%&quot; Next, we separate the giant cluster into more meaningful clusters, as determined by the spinglass clustering algorithm. We store the cluster information in csg0: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD library(tidyverse) create_yvar2 = function(yv1) { # Creates yvar2 as a linear outcome of yvar1 # Args: # yv1: yvar1 yv1 * .25 + rnorm(n = 1, mean = 10, sd = 100) } #------------------------------------------------------------------------------ # Make the dataset high_y_values &lt;- tibble( yvar1 = sample(1000:10000, 100, replace = TRUE)) %&gt;% mutate(yvar2 = map_dbl(yvar1, create_yvar2)) ## Error: &lt;text&gt;:1:1: unexpected input ## 1: &lt;&lt; ## ^ library(simstudy) ======= csg_0 &lt;- giant_cluster %&gt;% cluster_spinglass # creates the clusters; &#39;csg&#39; = cluster spinglass csg_0$membership %&gt;% unique %&gt;% length # number of clusters/communities/groups ## Error: &lt;text&gt;:2:1: unexpected &#39;==&#39; ## 1: library(simstudy) ## 2: == ## ^ 58d1f3d32df76c709efd19069c532b125f465a3d One of the important outcomes of this method is the modularity value \\(M\\). Modularity measures how good the division is, or how separated are the different vertex types from each other. The spinglass algorithm looks for the modularity of the optimal partition. For a given network, the partition with maximum modularity corresponds to the optimal community structure (i.e., a higher \\(M\\) is better). The maximum modularity score is +1 and according to Hogan (2017), networks with a modularity above 0.3 as “very modular,” meanig that most edges are within communities. Note also that if \\(M\\) = 0, all nodes belong to one group. &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD data1 &lt;- genData(500, def) data1 ======= csg_0$modularity &gt;&gt;&gt;&gt;&gt;&gt;&gt; 58d1f3d32df76c709efd19069c532b125f465a3d ## Error: &lt;text&gt;:2:1: unexpected &#39;&gt;&#39; ## 1: csg_0$modularity ## 2: &gt; ## ^ 17.5.1 Identifying the “typical” number of clusters returned with the spinglass algorithm It is important to note that a different result is returned each time the spinglass clustering algorithm is run. For this reason, we needed to run a number of simulations to see what the “typical” number of clusters are. We ran the algorithm 100 times and looked at the mean and median number of clusters obtained. We made a note of a seed that produced the median number of clusters, confirmed that this was reproducible, and then set this seed so that all future work will be run with this same clustering configuration. csg_matrix &lt;- matrix(NA, nrow=1, ncol=100) for (i in 1:100) { print(i) set.seed(i) csg = giant_cluster %&gt;% cluster_spinglass csg_matrix[1,i] &lt;- max(csg$membership) } ## [1] 1 ## Error in giant_cluster %&gt;% cluster_spinglass: could not find function &quot;%&gt;%&quot; csg_matrix_summary &lt;- csg_matrix %&gt;% length %&gt;% rbind(csg_matrix %&gt;% mean %&gt;% round(2)) %&gt;% rbind(csg_matrix %&gt;% sd %&gt;% round(2)) %&gt;% rbind(csg_matrix %&gt;% median) %&gt;% rbind(csg_matrix %&gt;% min) %&gt;% rbind(csg_matrix %&gt;% max) ## Error in csg_matrix %&gt;% length %&gt;% rbind(csg_matrix %&gt;% mean %&gt;% round(2)) %&gt;% : could not find function &quot;%&gt;%&quot; colnames(csg_matrix_summary) &lt;- c(&quot;&quot;) ## Error in colnames(csg_matrix_summary) &lt;- c(&quot;&quot;): object &#39;csg_matrix_summary&#39; not found rownames(csg_matrix_summary) &lt;- c(&quot;number of tests: &quot;, &quot;mean: &quot;, &quot;sd: &quot;, &quot;median: &quot;, &quot;min: &quot;, &quot;max: &quot;) ## Error in rownames(csg_matrix_summary) &lt;- c(&quot;number of tests: &quot;, &quot;mean: &quot;, : object &#39;csg_matrix_summary&#39; not found csg_matrix_summary ## Error in eval(expr, envir, enclos): object &#39;csg_matrix_summary&#39; not found ## select a seed from this list which reproduces the median number of clusters seeds &lt;-{as.vector(csg_matrix) == median(csg_matrix)} %&gt;% which ## Error in {: could not find function &quot;%&gt;%&quot; our_seed &lt;- seeds[1] ## Error in eval(expr, envir, enclos): object &#39;seeds&#39; not found set.seed(our_seed) # set the seed ## Error in set.seed(our_seed): object &#39;our_seed&#39; not found csg &lt;- giant_cluster %&gt;% cluster_spinglass ## Error in giant_cluster %&gt;% cluster_spinglass: could not find function &quot;%&gt;%&quot; csg_summary &lt;- csg$vcount %&gt;% rbind(giant_cluster %&gt;% gsize) %&gt;% rbind(csg$csize %&gt;% length) %&gt;% rbind(csg$modularity %&gt;% round(4)) ## Error in csg$vcount %&gt;% rbind(giant_cluster %&gt;% gsize) %&gt;% rbind(csg$csize %&gt;% : could not find function &quot;%&gt;%&quot; colnames(csg_summary) &lt;- c(&quot;&quot;) ## Error in colnames(csg_summary) &lt;- c(&quot;&quot;): object &#39;csg_summary&#39; not found rownames(csg_summary) &lt;- c(&quot;Number of nodes: &quot;, &quot;Number of edges: &quot;, &quot;Number of clusters: &quot;, &quot;Modularity: &quot;) ## Error in rownames(csg_summary) &lt;- c(&quot;Number of nodes: &quot;, &quot;Number of edges: &quot;, : object &#39;csg_summary&#39; not found csg_summary ## Error in eval(expr, envir, enclos): object &#39;csg_summary&#39; not found print(&quot;Size of each cluster: &quot;, quote=FALSE); print(csg$csize) ## [1] Size of each cluster: ## Error in print(csg$csize): object &#39;csg&#39; not found 17.5.2 Test of statistical significance for spinglass clusters The test for statistical significance for spinglass clustering is a bit different than the familiar tests that return \\(p\\)-values (Csardi, Nepusz, &amp; Airoldi (2016, pp. 132-138). The idea behind this test of significance is that a random network of equal size and degree distribution as our observed network should have a lower modularity score–that is, if the observed network does in fact have statistically significant clustering. The following R procedure generates 100 randomized instances of our network (with the same size and degree distribution) using the sample_ degseq() function. The method = 'vl' ensures that there are no loop edges in the randomly generated networks. We then applied the spinglass clustering algorithm to each of the 100 randomized instances of the network. A ‘0’ result from this procudure indicates that no randomized networks have community structure with a modularity score that is higher than the one obtained from the original, observed network. Hence a ‘0’ result means that our network has significant community structure; any non-zero results means that the detected spinglass clusters are not statistically significant. degrees &lt;- giant_cluster %&gt;% as.undirected %&gt;% degree(mode=&#39;all&#39;, loops=FALSE) ## Error in giant_cluster %&gt;% as.undirected %&gt;% degree(mode = &quot;all&quot;, loops = FALSE): could not find function &quot;%&gt;%&quot; qr_vl &lt;- replicate(100, sample_degseq(degrees, method=&quot;vl&quot;), simplify=FALSE) %&gt;% lapply(cluster_spinglass) %&gt;% sapply(modularity) ## Error in replicate(100, sample_degseq(degrees, method = &quot;vl&quot;), simplify = FALSE) %&gt;% : could not find function &quot;%&gt;%&quot; sum(qr_vl &gt; csg$modularity) / 100 ## Error in eval(expr, envir, enclos): object &#39;qr_vl&#39; not found 17.6 Network Visualization with Clusters Visualizations of social networks are interesting and powerful–and increasingly common. Here, we create a visualization of our network structure, using the color palette generated by our spinglass clustering. ## color-blind palette ## source: https://jacksonlab.agronomy.wisc.edu/2016/05/23/15-level-colorblind-friendly-palette/ palette &lt;- c(&quot;#000000&quot;,&quot;#004949&quot;,&quot;#009292&quot;,&quot;#ff6db6&quot;,&quot;#ffb6db&quot;, &quot;#490092&quot;,&quot;#006ddb&quot;,&quot;#b66dff&quot;,&quot;#6db6ff&quot;,&quot;#b6dbff&quot;, &quot;#920000&quot;,&quot;#924900&quot;,&quot;#db6d00&quot;,&quot;#24ff24&quot;,&quot;#ffff6d&quot;) csg_palette &lt;- palette[csg$membership] ## Error in eval(expr, envir, enclos): object &#39;csg&#39; not found Here, we used the Fruchterman-Reingold layout algorithm (layout = 'fr'), which is appropriate for large (but still with less than 1,000 nodes), potentially disconnected networks. library(ggraph) ## Error in library(ggraph): there is no package called &#39;ggraph&#39; library(gridExtra) ## Error in library(gridExtra): there is no package called &#39;gridExtra&#39; layout_randomly &lt;- giant_cluster %&gt;% create_layout(layout=&#39;randomly&#39;) ## Error in giant_cluster %&gt;% create_layout(layout = &quot;randomly&quot;): could not find function &quot;%&gt;%&quot; layout_mds &lt;- giant_cluster %&gt;% create_layout(layout=&#39;mds&#39;) ## Error in giant_cluster %&gt;% create_layout(layout = &quot;mds&quot;): could not find function &quot;%&gt;%&quot; layout_fr &lt;- giant_cluster %&gt;% create_layout(layout=&#39;fr&#39;) ## Error in giant_cluster %&gt;% create_layout(layout = &quot;fr&quot;): could not find function &quot;%&gt;%&quot; layout_drl &lt;- giant_cluster %&gt;% create_layout(layout=&#39;drl&#39;) ## Error in giant_cluster %&gt;% create_layout(layout = &quot;drl&quot;): could not find function &quot;%&gt;%&quot; layout_kk &lt;- giant_cluster %&gt;% create_layout(layout=&#39;kk&#39;) ## Error in giant_cluster %&gt;% create_layout(layout = &quot;kk&quot;): could not find function &quot;%&gt;%&quot; layout_sugiyama &lt;- giant_cluster %&gt;% create_layout(layout=&#39;sugiyama&#39;) ## Error in giant_cluster %&gt;% create_layout(layout = &quot;sugiyama&quot;): could not find function &quot;%&gt;%&quot; ## Additional layout algorithms to try: #layout_auto &lt;- giant_cluster %&gt;% create_layout(layout=&#39;nicely&#39;) #layout_lgl &lt;- giant_cluster %&gt;% create_layout(layout=&#39;lgl&#39;) #layout_dh &lt;- giant_cluster %&gt;% create_layout(layout=&#39;dh&#39;) #layout_graphopt &lt;- giant_cluster %&gt;% create_layout(layout=&#39;graphopt&#39;) viz_random &lt;- ggraph(layout_randomly) + geom_edge_link(width=.1, arrow = arrow(length=unit(1, &#39;mm&#39;))) + geom_node_point(alpha=.75, size=4, color=csg_palette) + theme_bw() + theme(plot.background = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), legend.position=&quot;none&quot; ) ## Error in ggraph(layout_randomly): could not find function &quot;ggraph&quot; viz_mds &lt;- ggraph(layout_mds) + geom_edge_link(width=.1, arrow = arrow(length=unit(1, &#39;mm&#39;))) + geom_node_point(alpha=.75, size=4, color=csg_palette) + theme_bw() + theme(plot.background = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), legend.position=&quot;none&quot; ) ## Error in ggraph(layout_mds): could not find function &quot;ggraph&quot; viz_fr &lt;- ggraph(layout_fr) + geom_edge_link(width=.1, arrow = arrow(length=unit(1, &#39;mm&#39;))) + geom_node_point(alpha=.75, size=4, color=csg_palette) + theme_bw() + theme(plot.background = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), legend.position=&quot;none&quot; ) ## Error in ggraph(layout_fr): could not find function &quot;ggraph&quot; viz_drl &lt;- ggraph(layout_drl) + geom_edge_link(width=.1, arrow = arrow(length=unit(1, &#39;mm&#39;))) + geom_node_point(alpha=.75, size=4, color=csg_palette) + theme_bw() + theme(plot.background = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), legend.position=&quot;none&quot; ) ## Error in ggraph(layout_drl): could not find function &quot;ggraph&quot; viz_kk &lt;- ggraph(layout_kk) + geom_edge_link(width=.1, arrow = arrow(length=unit(1, &#39;mm&#39;))) + geom_node_point(alpha=.75, size=4, color=csg_palette) + theme_bw() + theme(plot.background = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), legend.position=&quot;none&quot; ) ## Error in ggraph(layout_kk): could not find function &quot;ggraph&quot; viz_sugiyama &lt;- ggraph(layout_sugiyama) + geom_edge_link(width=.1, arrow = arrow(length=unit(1, &#39;mm&#39;))) + geom_node_point(alpha=.75, size=4, color=csg_palette) + theme_bw() + theme(plot.background = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), legend.position=&quot;none&quot; ) ## Error in ggraph(layout_sugiyama): could not find function &quot;ggraph&quot; grid.arrange(viz_random, viz_mds, viz_fr, viz_drl, viz_kk, viz_sugiyama, nrow = 3) ## Error in grid.arrange(viz_random, viz_mds, viz_fr, viz_drl, viz_kk, viz_sugiyama, : could not find function &quot;grid.arrange&quot; "],
["head.html", "Chapter 18 &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 18.1 Selection and influence 18.2 Creating example data in the form of an edgelist 18.3 An example of influence 18.4 Selection models 18.5 References", " Chapter 18 &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD The layouts above are as follows: the first row has 1.) a random plotting of nodes and 2.) the MDS algorithm. The second row has 3.) the FR algorithm and 4.) the DRL algorithm. The third row as 5.) the KK algorithm, and 6.) the Sugiyama algorithm. Each of these is appropriate in different situations; if unsure of which to use, the nicely option automatically selects an algorithm based on the network size. 18.1 Selection and influence Behind these visualizations, though, there are also statistical models and methods that can help to understand what is going on with respect to particular relationships in a network in additional ways. One way to consider these models and methods is in terms of two processes at play in our relationships (cite). These two processes are commonly (though not exclusively) the focus of statistical analyses of networks. In addition to not being exclusive, they do not interact independently: they affect each other reciprocally (Xu, Frank, &amp; Penuel, 2018). They are: Selection: the processes regarding who chooses to have a relationship with whom Infuence: the processes regarding how who we have relationships with affects our behavior While these are complex, they can be studied with the type of data collected from asking people about their relationships (and possibly asking them about or studying their behavior–or measuring some outcome). Happily, the use of these methods has expanded along with R: many of the best tools for studying social networks are in the form of long-standing R packages. Additionally, while there are many potential naunces to studying selection and influence, these are models that can fundamentally be carried out with regression, or the linear model (or extensions of it). In this walkthrough, the influence model is the focus. Nevertheless, we provide some direction for how to carry out selection modeling, too, at the end. 18.2 Creating example data in the form of an edgelist First, let’s create three different data frames. Here is what they should contain: A data frame indicating who the nominator and nominee for the relation (i.e., if Stefanie says that José is her friend, then Stefanie is the nominator and José the nominee) - as well as an optional variable indicating the weight, or strength, of their relation. This data frame and its type can be considered the basis for many types of social network analysis and is a common structure for network data: it is an edgelist. Data frames indicating the values of some behavior - an outcome - at two different time points. 18.3 An example of influence In this example, we create some example data that can be used to explore questions about how influence works. Note that Joshua Rosenberg and Sarah Galey initially wrote the following code for a walkthrough shared on Ken Frank’s website here. 58d1f3d32df76c709efd19069c532b125f465a3d Let’s take a look at the merged data. What this data now contains is the first data frame, data1, with each nominees’ outcome at time 1 (yvar1). Note that we will find each nominators’ outcome at time 2 later on. data &lt;- as_tibble(data) ## Error in as_tibble(data): could not find function &quot;as_tibble&quot; data ## function (..., list = character(), package = NULL, lib.loc = NULL, ## verbose = getOption(&quot;verbose&quot;), envir = .GlobalEnv, overwrite = TRUE) ## { ## fileExt &lt;- function(x) { ## db &lt;- grepl(&quot;\\\\.[^.]+\\\\.(gz|bz2|xz)$&quot;, x) ## ans &lt;- sub(&quot;.*\\\\.&quot;, &quot;&quot;, x) ## ans[db] &lt;- sub(&quot;.*\\\\.([^.]+\\\\.)(gz|bz2|xz)$&quot;, &quot;\\\\1\\\\2&quot;, ## x[db]) ## ans ## } ## names &lt;- c(as.character(substitute(list(...))[-1L]), list) ## if (!is.null(package)) { ## if (!is.character(package)) ## stop(&quot;&#39;package&#39; must be a character string or NULL&quot;) ## if (any(package %in% &quot;base&quot;)) ## warning(&quot;datasets have been moved from package &#39;base&#39; to package &#39;datasets&#39;&quot;) ## if (any(package %in% &quot;stats&quot;)) ## warning(&quot;datasets have been moved from package &#39;stats&#39; to package &#39;datasets&#39;&quot;) ## package[package %in% c(&quot;base&quot;, &quot;stats&quot;)] &lt;- &quot;datasets&quot; ## } ## paths &lt;- find.package(package, lib.loc, verbose = verbose) ## if (is.null(lib.loc)) ## paths &lt;- c(path.package(package, TRUE), if (!length(package)) getwd(), ## paths) ## paths &lt;- unique(normalizePath(paths[file.exists(paths)])) ## paths &lt;- paths[dir.exists(file.path(paths, &quot;data&quot;))] ## dataExts &lt;- tools:::.make_file_exts(&quot;data&quot;) ## if (length(names) == 0L) { ## db &lt;- matrix(character(), nrow = 0L, ncol = 4L) ## for (path in paths) { ## entries &lt;- NULL ## packageName &lt;- if (file_test(&quot;-f&quot;, file.path(path, ## &quot;DESCRIPTION&quot;))) ## basename(path) ## else &quot;.&quot; ## if (file_test(&quot;-f&quot;, INDEX &lt;- file.path(path, &quot;Meta&quot;, ## &quot;data.rds&quot;))) { ## entries &lt;- readRDS(INDEX) ## } ## else { ## dataDir &lt;- file.path(path, &quot;data&quot;) ## entries &lt;- tools::list_files_with_type(dataDir, ## &quot;data&quot;) ## if (length(entries)) { ## entries &lt;- unique(tools::file_path_sans_ext(basename(entries))) ## entries &lt;- cbind(entries, &quot;&quot;) ## } ## } ## if (NROW(entries)) { ## if (is.matrix(entries) &amp;&amp; ncol(entries) == 2L) ## db &lt;- rbind(db, cbind(packageName, dirname(path), ## entries)) ## else warning(gettextf(&quot;data index for package %s is invalid and will be ignored&quot;, ## sQuote(packageName)), domain = NA, call. = FALSE) ## } ## } ## colnames(db) &lt;- c(&quot;Package&quot;, &quot;LibPath&quot;, &quot;Item&quot;, &quot;Title&quot;) ## footer &lt;- if (missing(package)) ## paste0(&quot;Use &quot;, sQuote(paste(&quot;data(package =&quot;, &quot;.packages(all.available = TRUE))&quot;)), ## &quot;\\n&quot;, &quot;to list the data sets in all *available* packages.&quot;) ## else NULL ## y &lt;- list(title = &quot;Data sets&quot;, header = NULL, results = db, ## footer = footer) ## class(y) &lt;- &quot;packageIQR&quot; ## return(y) ## } ## paths &lt;- file.path(paths, &quot;data&quot;) ## for (name in names) { ## found &lt;- FALSE ## for (p in paths) { ## tmp_env &lt;- if (overwrite) ## envir ## else new.env() ## if (file_test(&quot;-f&quot;, file.path(p, &quot;Rdata.rds&quot;))) { ## rds &lt;- readRDS(file.path(p, &quot;Rdata.rds&quot;)) ## if (name %in% names(rds)) { ## found &lt;- TRUE ## if (verbose) ## message(sprintf(&quot;name=%s:\\t found in Rdata.rds&quot;, ## name), domain = NA) ## thispkg &lt;- sub(&quot;.*/([^/]*)/data$&quot;, &quot;\\\\1&quot;, p) ## thispkg &lt;- sub(&quot;_.*$&quot;, &quot;&quot;, thispkg) ## thispkg &lt;- paste0(&quot;package:&quot;, thispkg) ## objs &lt;- rds[[name]] ## lazyLoad(file.path(p, &quot;Rdata&quot;), envir = tmp_env, ## filter = function(x) x %in% objs) ## break ## } ## else if (verbose) ## message(sprintf(&quot;name=%s:\\t NOT found in names() of Rdata.rds, i.e.,\\n\\t%s\\n&quot;, ## name, paste(names(rds), collapse = &quot;,&quot;)), ## domain = NA) ## } ## if (file_test(&quot;-f&quot;, file.path(p, &quot;Rdata.zip&quot;))) { ## warning(&quot;zipped data found for package &quot;, sQuote(basename(dirname(p))), ## &quot;.\\nThat is defunct, so please re-install the package.&quot;, ## domain = NA) ## if (file_test(&quot;-f&quot;, fp &lt;- file.path(p, &quot;filelist&quot;))) ## files &lt;- file.path(p, scan(fp, what = &quot;&quot;, quiet = TRUE)) ## else { ## warning(gettextf(&quot;file &#39;filelist&#39; is missing for directory %s&quot;, ## sQuote(p)), domain = NA) ## next ## } ## } ## else { ## files &lt;- list.files(p, full.names = TRUE) ## } ## files &lt;- files[grep(name, files, fixed = TRUE)] ## if (length(files) &gt; 1L) { ## o &lt;- match(fileExt(files), dataExts, nomatch = 100L) ## paths0 &lt;- dirname(files) ## paths0 &lt;- factor(paths0, levels = unique(paths0)) ## files &lt;- files[order(paths0, o)] ## } ## if (length(files)) { ## for (file in files) { ## if (verbose) ## message(&quot;name=&quot;, name, &quot;:\\t file= ...&quot;, .Platform$file.sep, ## basename(file), &quot;::\\t&quot;, appendLF = FALSE, ## domain = NA) ## ext &lt;- fileExt(file) ## if (basename(file) != paste0(name, &quot;.&quot;, ext)) ## found &lt;- FALSE ## else { ## found &lt;- TRUE ## zfile &lt;- file ## zipname &lt;- file.path(dirname(file), &quot;Rdata.zip&quot;) ## if (file.exists(zipname)) { ## Rdatadir &lt;- tempfile(&quot;Rdata&quot;) ## dir.create(Rdatadir, showWarnings = FALSE) ## topic &lt;- basename(file) ## rc &lt;- .External(C_unzip, zipname, topic, ## Rdatadir, FALSE, TRUE, FALSE, FALSE) ## if (rc == 0L) ## zfile &lt;- file.path(Rdatadir, topic) ## } ## if (zfile != file) ## on.exit(unlink(zfile)) ## switch(ext, R = , r = { ## library(&quot;utils&quot;) ## sys.source(zfile, chdir = TRUE, envir = tmp_env) ## }, RData = , rdata = , rda = load(zfile, ## envir = tmp_env), TXT = , txt = , tab = , ## tab.gz = , tab.bz2 = , tab.xz = , txt.gz = , ## txt.bz2 = , txt.xz = assign(name, read.table(zfile, ## header = TRUE, as.is = FALSE), envir = tmp_env), ## CSV = , csv = , csv.gz = , csv.bz2 = , ## csv.xz = assign(name, read.table(zfile, ## header = TRUE, sep = &quot;;&quot;, as.is = FALSE), ## envir = tmp_env), found &lt;- FALSE) ## } ## if (found) ## break ## } ## if (verbose) ## message(if (!found) ## &quot;*NOT* &quot;, &quot;found&quot;, domain = NA) ## } ## if (found) ## break ## } ## if (!found) { ## warning(gettextf(&quot;data set %s not found&quot;, sQuote(name)), ## domain = NA) ## } ## else if (!overwrite) { ## for (o in ls(envir = tmp_env, all.names = TRUE)) { ## if (exists(o, envir = envir, inherits = FALSE)) ## warning(gettextf(&quot;an object named %s already exists and will not be overwritten&quot;, ## sQuote(o))) ## else assign(o, get(o, envir = tmp_env, inherits = FALSE), ## envir = envir) ## } ## rm(tmp_env) ## } ## } ## invisible(names) ## } ## &lt;bytecode: 0x7fdd2e2a42f0&gt; ## &lt;environment: namespace:utils&gt; 18.3.1 Calculating an exposure term This is the key step that makes this model - a regression, or linear, model - one that is special. It is creating an exposure term. The idea is that the exposure term “captures” how your interactions with someone, over some period of time (between the first and second time points) impact some outcome. This model accounts for an individual’s initial report of the outcome, i.e., their time 1 prior value, so it is a model for change in some outcome. # Calculating exposure data$exposure &lt;- data$relate * data$yvar1 # Calculating mean exposure mean_exposure &lt;- data %&gt;% group_by(nominator) %&gt;% summarize(exposure_mean = mean(exposure)) &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD ======= &gt;&gt;&gt;&gt;&gt;&gt;&gt; 58d1f3d32df76c709efd19069c532b125f465a3d ## Error: &lt;text&gt;:8:1: unexpected input ## 7: summarize(exposure_mean = mean(exposure)) ## 8: &lt;&lt; ## ^ What this data frame - mean_exposure - contains is the mean of the outcome (in this case, yvar1) for all of the individuals the nominator had a relation with. As we need a final data set with mean_exposure, mean_exposure_plus, degree, yvar1, and yvar2 added, we’ll process the data a bit more. mean_exposure_terms &lt;- left_join(mean_exposure, mean_exposure_plus, by = &quot;nominator&quot;) ## Error in left_join(mean_exposure, mean_exposure_plus, by = &quot;nominator&quot;): could not find function &quot;left_join&quot; names(data2) &lt;- c(&quot;nominator&quot;, &quot;yvar1&quot;) # rename nominee as nominator to merge these ## Error in names(data2) &lt;- c(&quot;nominator&quot;, &quot;yvar1&quot;): object &#39;data2&#39; not found final_data &lt;- left_join(mean_exposure_terms, data2, by = &quot;nominator&quot;) ## Error in left_join(mean_exposure_terms, data2, by = &quot;nominator&quot;): could not find function &quot;left_join&quot; final_data &lt;- left_join(final_data, data3, by = &quot;nominator&quot;) # data3 already has nominator, so no need to change ## Error in left_join(final_data, data3, by = &quot;nominator&quot;): could not find function &quot;left_join&quot; 18.3.2 Regression (linear models) Calculating the exposure term is the most distinctive and important step in carrying out influence models. Now, we can simply use a linear model to find out how much relations - as captured by the influence term - affect some outcome. model1 &lt;- lm(yvar2 ~ yvar1 + exposure_mean, data = final_data) ## Error in is.data.frame(data): object &#39;final_data&#39; not found summary(model1) ## Error in summary(model1): object &#39;model1&#39; not found Note that these models show … [add] So, the influence model is used to study a key process for social network analysis, but it is one that is useful, because you can quantify, given what you measure and how you measure it, the network effect, something that is sometimes not considered, especially in education (Sweet, 2017). It’s also fundamentally a regression. That’s really it, as the majority of the work goes into calculating the exposure term. 18.4 Selection models While this tutorial focused on influence models, selection models are also commonly used - and are commonly of interest not only to researchers but also to administrators and teachers (and even to youth and students). Here, we briefly describe a few possible approaches for using a selection model. At its core, the selection model is a regression - albeit, one that is a generalization of one, namely, a logistic regression (sometimes termed a generalized linear model, because it is basically a regression but is one with an outcome that consists just of 0’s and 1’s). Thus, the most straight-away way to use a selection model is to use a logistic regression where all of the relations (note the relate variable in data1 above) are indicated with a 1. But, here is the important and challenging step: all of the possible relations (i.e., all of the relations that are possible between all of the individuals in a network) are indicated with a 0 in an edgelist. Note that, again, an edgelist is the preferred data structure for carrying out this analysis. This step involves some data wrangling, especially the idea of widening or lengthening a data frame. Once all of the relations are indicated with a 1 or a 0, then a simple linear regression can be used. Imagine that we are interested in whether individuals from the same group are more or less likely to interact than those from different groups; same could be created in the data frame based upon knowing which group both nominator and nominee are from: m_selection &lt;- glm(relate ~ 1 + same, data = edgelist1) While this is a straightforward way to carry out a selection model, there are some limitations to it. Namely, it does not account for individuals who send more (or less) nominations overall–and not considering this may mean other effects, like the one associated with being from the same group, are not accurate. A few extensions of the linear model - including those that can use data for which relationships are indicated with weights, not just 1’s and 0’s, have been developed. One type of model extends the logistic regression. It can be used for data that is not only 1’s and 0’s but also data that is normally distributed or has fixed-ranks. It is the amen package available here. A particularly common one is an Exponential Random Graph Model, or an ERGM. An R package that makes estimating these easy is available here. That R package, ergm, is part of a powerful and often-used collection of packages, including those for working with network data (data that can begin with an edgelist, but may need additional processing that is challenging to do with edgelist data), statnet. A link to the statnet packages is here. 18.5 References Bates, D., Maechler, M., Bolker, B., &amp; Walker, S. (2018). lme4: Linear mixed-effects models using ‘Eigen’ and S4 (Version 1.1-19) [R package]. Retrieved from https://cran.r-project.org/package=lme4 Csardi, G. (2018). igraph: Network analysis and visualization (Version 1.2.2) [R package]. Retrieved from https://CRAN.R-project.org/package=igraph Csardi, G., Nepusz, T., &amp; Airoldi, E. M. (2016). Statistical network analysis with igraph. New York, NY: Springer. Retrieved from https://sites.fas.harvard.edu/~airoldi/pub/books/BookDraft-CsardiNepuszAiroldi2016.pdf Hogan, B. (2017). Online social networks: Concepts for data collection and analysis. In N. G. Fielding, R. M. Lee, &amp; G. Blank (Eds.), The SAGE handbook of online research methods (2nd ed., pp. ). London, UK: SAGE. Kadushin, C. (2012). Understanding social networks: Theories, concepts, and findings. New York, NY: Oxford University Press. Pedersen, T. L. (2018). ggraph: An implementation of grammar of graphics for graphs and networks (Version 1.0.2) [R package]. Retrieved from https://CRAN.R-project.org/package=ggraph R Core Team. (2018). R: A language and environment for statistical computing (Version 3.5.0) [Computer software]. Vienna, Austria: R Foundation for Statistical Computing. Retrieved from https://www.R-project.org/ Reichardt, J., &amp; Bornholdt, S. (2006). Statistical mechanics of community detection. Physical Review E, 74(1), 016110. Retrieved from https://arxiv.org/abs/cond-mat/0603718 Wickham, H., Chang, W., &amp; RStudio. (2016). ggplot2: Create elegant data visualisations using the grammar of graphics (Version 2.2.1) [R package]. Retrieved from https://CRAN.R-project.org/package=ggplot2 Wickham, H., Francois, R., Henry, L., Muller, K., &amp; RStudio. (2018). dplyr: A grammar of data manipulation (Version 0.7.6) [R package]. Retrieved from https://CRAN.R-project.org/package=dplyr "],
["background-4.html", "Chapter 19 Background", " Chapter 19 Background A common situation encountered when using data for analyzing the education sector, particularly for analysts who are not directly working with schools or districts, is the prevalence of aggregate data. Aggregate data refers to numerical or non-numerical information that is (1) collected from multiple sources and/or on multiple measures, variables, or individuals and (2) compiled into data summaries or summary reports, typically for the purposes of public reporting or statistical analysis. Example of publicly available aggregate data include school-level graduation rates or state test proficiency scores by grade and subject. These datasets are large, lagging, and often suppressed to protect privacy. Because of their coarseness, they can be difficult to use in decision-making. However, these datasets are available to analysts for gleaning insights about education and can be used for landscape analyses or to supplement other analyses. "],
["the-data.html", "Chapter 20 The Data", " Chapter 20 The Data 20.0.1 Understanding Trends 20.0.2 Making Comparisons 20.0.3 Finding Inequities "],
["advanced-uses.html", "Chapter 21 Advanced Uses 21.1 Multi-level models 21.2 Text analysis 21.3 Longitudinal analysis", " Chapter 21 Advanced Uses 21.1 Multi-level models This is an example from the online science motivation dataset. d &lt;- readr::read_csv(&quot;data/online-science-motivation.csv&quot;) ## Error: &#39;data/online-science-motivation.csv&#39; does not exist in current working directory (&#39;/Users/jessemostipak/Documents/01.ds_edu/data-science-in-education&#39;). d ## Error in eval(expr, envir, enclos): object &#39;d&#39; not found 21.2 Text analysis 21.3 Longitudinal analysis "],
["solutions-for-adopting-data-science-techniques-in-education.html", "Chapter 22 Solutions for Adopting Data Science Techniques in Education", " Chapter 22 Solutions for Adopting Data Science Techniques in Education "],
["students-doing-data-science.html", "Chapter 23 Students doing data science", " Chapter 23 Students doing data science "],
["resources-used-in-the-creation-of-this-text.html", "Chapter 24 Resources used in the creation of this text", " Chapter 24 Resources used in the creation of this text National Center for Research in Policy and Practice: Findings from a National Study on Research Use Among School and District Leaders Summary: A survey of 733 school principals and district leaders within US mid-sized and large school districts, focused on how educational leaders use research to inform their decision-making. "],
["learning-more.html", "Chapter 25 Learning More 25.1 How to learn more: Work in the open 25.2 Sharing one’s work 25.3 Asking for and receiving help 25.4 Books and Resources 25.5 Courses and Communities", " Chapter 25 Learning More While this book aims to serve as an introduction to data science in education, there is much more to learn. 25.1 How to learn more: Work in the open Sharing one’s work in the open is a great way to learn more. We discuss how welcoming others, sharing one’s work, asking for and providing help, and taking a growth mindset are ways to learn more 25.1.1 Welcoming others One strength of the data science community (at least the corner of it that the authors of this book occupy) is that it can be a welcoming place. To the extent that the data science community is this - a welcoming place - it is due to the hard work of many of the people in the data science community. In order for the data science community to continue to be welcoming - and to be more welcoming to those for whom its currently not - it’s important for all of us to continue to build a data science community that is inclusive of those who wish to be involved. Especially to those of us in education, there are many opportunities to welcome others. One thing to be aware of: You may end up liking data science so much that you become an evangelist (and may, if you are like us, on occasion, make people think you are a bit crazy for R!). This is one thing to keep in mind as regards welcoming others: People will want to use data science at different times and for different reasons. Keeping this in mind can keep one’s focus on welcoming those who are already interested and ready to learn more. Welcoming others has the side benefit of positioning you as someone with (even if it is still-developing) expertise in data science and R. Teaching others - or even helping others to get started - is a great way to learn more about one’s understanding and to learn more in the process. 25.2 Sharing one’s work A great way to learn more is to share one’s work in the open. The first concern that many of us have about sharing our work in the open is that it is imperfect. This is a dilemma that everyone manages differently; some data scientist’s blog posts are very polished and article-like, whereas others are comfortable sharing ideas-in-progress or shorter posts. Especially for sharing work related to data science, there are blogging platforms that make it easy to publish code, its output, and text. Hill has a great introduction to getting up and running with Blogdown here. 25.3 Asking for and receiving help There are a number of platforms for asking for and receiving help that can be a great place to learn more. Questions and answers from sites such as R Studio Community and Stack Overflow are often among the first items that are returned from a search on a topic. Asking one’s own question on these platforms is a great way to get help specific to the problem you are facing; one helpful strategy when asking a question is to include a reprex, a reproducible example. See Bryan’s page with resources related to reprexes here. Of course, answering questions is also a great way to help others - and to learn more about the challenges others face or how to approach specific problems in the process. 25.3.1 Adopting a growth mindset Doing data science and using R are hard. Moreover, both are changing rapidly and it can feel difficult to keep up. We suggest taking a growth mindset to learning data science. In Dweck’s characterization, when it comes to our capabilities, we can think of ourselves as being (and not being) mathematics or reading people, with this being an unchangeable part of who we are. Conversely, we can think of ourselves as being more or less capable depending on how much we want to be, who we are around, and how much effort we exert. The former way of thinking is a fixed mindset, whereas the latter is a growth mindset. As educators, we would be remiss not to mention some of the critiques (and concerns) about focusing exclusively on a growth mindset (see this post here), but it can be a useful idea when used narrowly and as a spark to push further and to learn more. We would like to suggest that having a growth mindset is especially helpful for learning more about data science. Partially, this is because what data science is is still being discussed: and so nobody is born a data scientist! More importantly, individuals come to data science - and make contributions to data science - with many different backgrounds, many of them unrelated to STEM (Science, Technology, Engineering, and Mathematics) backgrounds. This is a strength of the data science field as it exists now and we think this should be nurtured. 25.4 Books and Resources There are some books and resources we recommend. Doing data science with R by Wickham and Grolemund (2017) Big magic with R: Creating learning beyond fear by Hill (2017) 25.5 Courses and Communities There are also some courses and online communities we recommend. #r4ds (see a talk at rstudio::conf() here by Maegan (2019)) Data science for social scientists by Landers (2019) University of Oregon Data Science Specialization for the College of Education by Anderson (2019) "]
]
