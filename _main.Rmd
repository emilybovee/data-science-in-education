# Introduction 

Dear Data Scientists, Educators, and Data Scientists who are Educators: 

This book is a warm welcome and an invitation. If you're a data scientist in education or an educator in data science, we know that your role isn't exactly straightforward. We welcome everyone who wants to understand data science in education better. 

If you work in education or data science, you also own a part of the solution. We invite everyone to help define what it means to practice data science in education by sharing their experiences. 

### The Challenge of Data Science in Education 

We'll get to work on understanding data science in education soon, but first let's talk about why this relationship is not such a straightforward thing. 

Talking about data science in education is hard because everyone tackles it on different levels. If education were a building, it would be multi-storied with many rooms. There are privately and publicly funded schools. There are more than eighteen possible grade levels. You can be educated alone in front of a computer or with others in a classroom. 

This imaginary building also has rooms the residents never see: Business and finance staff plan for efficient use of limited funds. The transportation department plans bus routes across vast spaces. University administrators search for the best way to measure career readiness. 

So why don't we see more data science happening in these areas of education? Data science is a relatively new field. This means that our community is still trying to work out how it all fits in. It also means that folks in education aren't always used to having someone around [who understands education, knows how to code, and can use statistical techniques](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram) all at once. 

### Meeting the Challenge 

As the data science field grows, we'll need better language to describe what it means in education and how to use it to meet our goals for students. In this book we want to take a step towards understanding data science in education better by exploring challenges you're likely to encounter no matter how you work with data in education. After that we describe basic and advanced data science skills that you can use to tackle these challenges. Finally, we'll present walkthroughs of analyses conducted in the education setting to bring these challenges and techniques to life. 

We hope after reading this book you'll feel like you're not alone in defining how to do data science in your education job. We also hope the techniques and examples here give you ideas to kickstart using data science to meet your goals in education. Finally, we hope you accept our invitation to contribute to this work by sharing your own challenges and solutions.

<!--chapter:end:01-intro.Rmd-->

# How to Use This Book 

It is really hard to draw clean boundaries around the topic of data science in 
education because people are educated in all kinds of settings and in all kinds 
of age groups. Education organizations require different kinds of staff to 
make it work, which means different kinds of data science uses. A teacher's 
approach to learning from data is different from an administrator's or an 
opeartions manager. 

Since there are many different readers, we believe there should be 
different ways to use the book, both as a reader and as a contributor. Here are 
some ways to use this book:   

**Read the Book Cover to Cover**

Reading the book all the way through will give a nice high level view of data 
science in education, starting from the unique challenges of using data science 
in education and ending with code for example analyses. 

**Pick a Chapter That is Useful for Your Level of Experience and Start There** 

If you are a student or if you work in education, you may be trying to solve a 
very specific problem with data, like analyzing student quiz scores, projecting 
classroom sizes, or pitching a new data analysis method. In this case it might 
be useful to jump ahead to a chapter or section that discusses your area of 
interest. 

**Read Through the Walkthroughs and Run the Code**

If you are here to learn and practice coding in R, you can work through the 
example analyses. We wrote these based on typical data problems you might find 
as a student or staff in education, so it is worthwhile to copy or type the code, 
run it in your console, and change it to experiment with the results. 

**Contribute to the Book**

We quickly learned when planning the book that there are many ways to approach 
this topic and still we wanted to write in a way that is directly useful and 
practical for our readers in education. One way to meet this goal is to build 
procedures into the work for readers to directly contribute. We hope that as the 
book evolves it grows to reflect the observable needs of data scientists in 
education. Here are some ways readers can contribute: 

 - Submit a pull request to our 
 [GitHub site](https://github.com/jrosen48/data-science-in-education) that 
 describes a data science problem that is unique to the education setting 
 - Submit a pull request to share a solution for the problems discussed in the 
 book
 to the education setting
 - Share an anonymized dataset 

<!--chapter:end:02-how-to-use.Rmd-->

What is a Data Scientist in Education? 

One way to define data science is to think of it as [combining three skills](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram) to do data analysis: programming, statistics, and content knowledge. Though if you Google "what is a data scientist" you'll won't find a simple answer. 

But for this book's exploration, thinking of data science as a combination of these three skills is useful because we can try substituting the field of education in for "content knowledge." Even then, we still face a broad field of possibilities when imagining what a data scientist in education actually does on a day-to-day basis. 

While having no established data science identity makes it hard for educators to explain their data work to the layperson, it does allow them to take on a variety of data-related activities and, ultimately, build the definition of the role. So rather than grapple with defining this role, let's share some examples of what data scientists do in the field of education. 

*Leading Office Culture Toward a Data-Driven Approach*

Jesse, a director at an education non-profit in Texas, is setting up a database to house student achievement data. This project requires a number of data science skills we'll discuss in chapter five, including cleaning data into a consistent format. Once the data is prepared, Jesse builds dashboards to help her teammates explore the data.

But not all of Jesse's work can be found in a how-to manual for data scientists. She manages a team and serves as the de facto project manager for IT initiatives. And given her expertise and experience in data science, she's leading the charge towards a more data-driven approach within the organization.

*Helping School Districts Plan to Meet Their Goals*

Ryan, a special education administrator in California, uses data science to reproduce the state department of education's special education compliance metrics, then uses the results to build an early warning system for compliance based on local datasets. In this case, Ryan uses foundational data science skills like data cleaning, visualization, and modeling to help school districts monitor and meet their compliance requirements. 

*Doing and Empowering Research On Data Scienctists in Education*

Joshua, Assistant Professor of STEM Education at University of Tennessee in Knoxville, researches how students do data science and helps teachers teach the next generation of data-informed citizens. He makes this work possible by building R packages—self-contained groups of data tools—that he and other researchers use to analyze datasets efficiently. 

The data scientists in these examples apply statistics and programming to create new knowledge in the education field. But that's as far as we can go when looking for commonalities in their day-to-day work. Maybe the education community will develop common norms and expectations for how it all works together as the relationship between data science and education grows. 

But because this relationship is still young, it is important that the people growing data science within education understand the culture and unique challenges in their education job. Afterall, the defining feature that will differentiate data science in education from data science in general will be doing data science that meets the unique needs of students, staff, and administration in education.

<!--chapter:end:03-ds-role.Rmd-->

Because data science in the school setting is a relatively new phenomena, it's understandable that school staff may be wary of how data is collected and analyzed. It's common for school staff to question how data is used, particularly if the data is used to describe staff and student performance. School systems that want to evolve their data analysis processes into something practical and meaningful to student progress will need to do the difficult work of addressing these worries. 

# A Reproducible Approach

One way to do this is to build analytic processes that are open about what data is collected, how it is collected, how it is analyzed, and how it is considered alongside other data when used in decision-making conversations. This can be achieved through a number of activities, including regular conversations about analytic methods, written reports describing data collection, and receiving input about analytic goals from staff members. 

One such process for achieiving openess in data collection and analysis is called reproducible research. The concept of [Reproducible work]("https://en.wikipedia.org/wiki/Reproducibility#Reproducible_research") is the idea that a completed analysis should come with all the necessary materials, including a description of methodology and programming code, needed for someone else to run the analysis and achieve the same results. If school staff are apprehensive about how school data is collected and used, it follows that a more transparent method for using data could go some way towards putting school staff--the consumers of school data--at ease. 

# A Self-Driven Analytic Approach 

An organization should encourage their staff to do their own data analyses primarily for the purpose of testing their own hypotheses about student learning in their classrooms and to directly guide decisions about how they deliver instruction. There are at least two benefits to this approach. First, staff begin to realize the value of doing data analysis as an ongoing inquiry into their outcomes, instead of a special event once a year ahead of school board presentations. Second--and more important for the idea of reducing apprehension around data analysis in schools--school staff begin to demystify data analysis as a process. When school staff collect and analyze their own data, they know exactly how it is collected and exactly how it is analyzed. The long-term effect of this self-driven analytic approach might be more openess to analysis, whether it is self-driven or conducted by the school district.

Building and establishing data governance that advocates for an open and transparent analytic process is difficult and long-term work, but the result will be less apprehension about how data is used and more channels for school staff to participate in the analysis. Here are more practical steps a school district can take towards building a more open approach to analysis: 

 - Make technical write-ups available so interested parties can learn more about how data was collected and analyzed 
 - Make datasets available to staff within the organization, to the extent that privacy laws and policies allow 
 - Establish an expectation that analysts present their work in a way that is accessible to many levels of data experience 
 - Hold regular forums to discuss how the organization collects and uses data

# Unique challenges

Educational data science is a new domain. It presents opportunities, like those discussed in the [previous chapter](#03-ds-role.Rmd), but also some challenges. These challenges vary a lot: We consider doing data science in education to include not only access, processing, and modeling data, but also social and cultural factors, like the training and support that educational data scientists have available to them. These challenges, then, range from the very general (and common to *all* domains in which data science is carried out) to very particular to educational data science. These are discussed in the remainder of this chapter.

## Challenges common to doing data science in any domain 

One challenge for educational data scientists is common to data scientists in other domains: Combining content knowledge, programming, and statistics to solve problems is a fairly new idea. In particular, the amount of data now available means that programming is often not only helpful, but necessary, for stakeholders to use data in education. Programming is powerful, but challenging; many of us in education do not have prior experience with it. Despite this challenge and the difficulty of writing the first few lines of code, there is good evidence, and many examples, that even those of us without prior programming experience can learn. 

## Lack of processes and procedures 

Other challenges are more about the process and practice of doing educational data science. Education is a field that is rich with data: survey, assessment, written, and policy and evaluation data, just for a few examples. Nevertheless, sometimes, there is a lack of processes and procedures in place for school districts and those working in them to share data with each other in order to build knowledge and context. Moreover, in academic and research settings, there are not often structures in place to facilitate the analysis of data and sharing of results. 
 
## Few guidelines from research and evaluation

While there is a body of past research on *students*' work with data (see Lee & Wilkerson, 2018, for a review), there is limited information from case- or design-based research on how others--teachers, administrators, and data scientists--use data in their work. In other words, we do not have a good idea for what best practices in our field are. This challenge is reflected in part in the variability in the roles of those who work with data. Many districts employ data analysts and research associates; some are now advertising and hiring for data scientist positions. 

## Limited training and educational opportunities for educational data science

Educational data science is new. At the present time, there are limited opportunities for those working in education to build their capabilities in educational data science (though this is changing to an extent; see Anderson and colleagues' work to create an educational data science certificate program at the University of Oregon and Bakers' educational data mining Massive Open Online Course offered through Coursera). 

Many educational data scientists have been trained in fields other than statistics, business analytics, or research. Moreover, the training in terms of particular tools and approaches that educational data scientists are highly varied. 
    
## The complex and messy nature of educational data

Another challenge concerns the particular nature of educational data. Educational data are often hierarchical, in that data at multiple "levels" is collected. These levels include classrooms, schools, districts, states, and countries - quite the hierarchy! In addition to the hierarchical nature of educational data, by their nature, these data often require linking with other data, such as data that provides context at each of the aforementioned levels. For example, when data is collected on students at the school level, it is often important to know about the training of the teachers in the school; data at the district level needs to be interpreted in the context of the funding provided by the community in terms of per-pupil spending and other, for example. A final aspect concerns the *type* of data collected. Often, educational data is numeric, but just as often, it is not: It involves characteristics of students, teachers, and other individuals that are categorical; open-ended responses that are strings; or even recordings that consist of audio and video data. All of these present challenges to the educational data scientist. 
    
## Ethical and legal concerns

Related to the complex and messy nature of educational data is its confidential nature. At the K-12 level, most data requires protections because of its human subjects focus, particularly because the data is about a protected population, youth. A closely related issue concerns the aims of education. Those working in education often seek to improve it and often work to do so with a scarcity of school and community resources. These ethical, legal, and even values-related concerns may become amplified as the role of data in education increases. They should be carefully considered and emphasized from the outset by those involved in educational data science.

## Analytic challenges

Due to the challenging nature of educational data, analyzing educational data is hard, too. The data is often not ready to be used: It may be in a format that is difficult to open without specialized software or it may need to be "cleaned" before it is usable. Closely related to the ethical and legal challenges, educational data scientists should be conscious of potential racial and gender biases in school models, and challenge not reinforce them. Because of the different *types* of data, the educational data scientist must often use a variety of analytic approaches, such as multi-level models, models for longitudinal data, or even models and analytic approaches for text data. 

<!--chapter:end:04-unique-challenges.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---

```{r, setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, out.width = "80%")
```

# Foundational Skills

This chapter is organized into two tracks (though, of course, you are welcome to read both). If you have experience using R - or have used it a few times, attended a workshop, or been involved with a collaborator who used it - consider starting with **Track Two**, focused on 'data loading and manipulation using the tidyverse', which covers reading/saving files, pipes, selecting, filtering, etc.  chapters. Otherwise, start at **Track One**, which covers installation, projects, and packages--and then proceed to the second track.

## Track One: Getting Started

First, you will need to download the latest versions of R and R Studio. R is a free environment for statistical computing and graphics using the programming language R. R Studio is a set of integrated tools that allows for a more user-friendly experience for using R.

Although you will likely use R Studio as your main console and editor, you must first install R as R Studio uses R behind-the-scenes. Both are freely-available, cross-platform, and open-source.

## Downloading R and R Studio

### To download R:

- Visit this page to download R: [https://cran.r-project.org/](https://cran.r-project.org/)
- Find your operating system (Mac, Windows, or Linux)
- Download the 'latest release' on the page for your operating system and download and install the application

Don't worry; you will not mess anything up if you download (or even install!) the wrong file. Once you've installed both, you can get started.

### To download R Studio:

- Visit this page to download R studio: [https://www.rstudio.com/products/rstudio/download/](https://www.rstudio.com/products/rstudio/download/)
- Find your operating system (Mac, Windows, or Linux)
- Download the 'latest release' on the page for your operating system and download and install the application

If you do have issues, consider this [page](https://datacarpentry.org/R-ecology-lesson/), and then reach out for help. One good place to start is the [R Studio Community](https://community.rstudio.com/) is a great place to start.

## Check that it worked

Open R Studio.  Find the console window and type in `2 + 2`. If what you can guess is returned (hint: it's what you expect!), then R Studio *and* R both work.

## Help, I'm completely new to using R / R Studio!

If you're completely new, Swirl is a great place to start, as it helps you to learn R *from within R Studio*. Visit this page to see some directions: [http://swirlstats.com](http://swirlstats.com).

And if you're ready to go, please proceed to the next sections on processing and preparing, plotting, loading, and modeling data and sharing results.

## Creating Projects

Before proceeding, we're going to take a few steps to set ourselves to make the analysis easier; namely, through the use of Projects, an R Studio-specific organizational tool. 

To create a project, in R Studio, navigate to "File" and then "New Directory". 

Then, click "New Project". Choose a directory name for the project that helps you to remember that this is a project that involves data science in education; it can be convenient if the name is typed in `lower-case-letters-separated-by-dashes`, like that. You can also choose the sub-directory. If you are just using this to learn and to test out creating a project, you may consider placing it in your downloads or another temporary directory so that you remember to remove it later.

Even if you do not create a Project, you can always check where your working directory (i.e., where your R is pointing) is by running `getwd()`. To change it manually, run `setwd(desired/file/path/here)`.

## Packages

"Packages" are shareable collections of R code that provide functions (i.e., a command to perform a specific task), data and documentation,. Packages increase the functionality of R by improving and expanding on base R (basic R functions). 

### Installing and Loading Packages

To download a package, you must call `install.packages()`:

```{r, eval = FALSE}
install.packages("dplyr", repos = "http://cran.us.r-project.org")
```

You can also navigate to the Packages pane, and then click "Install", which will work the same as the line of code above. This is a way to install a package using code or part of the R Studio interface. Usually, writing code is a bit quicker, but using the interface can be very useful and complimentary to use of code. 

*After* the package is installed, it must be loaded into your R Studio session using `library()`:

```{r}
library(dplyr)
```

We only have to install a package once, but to use it, we have to load it each time we start a new R session.

> a package is a like a book, a library is like a library; you use library() to check a package out of the library
> - Hadley Wickham, Chief Scientist, R Studio

### Running Functions from Packages

Once you have loaded the package in your session, you can run the functions that are contained within that package. To find a list of all those functions, you can run this in the R Studio console:

```{r}
help(package = dplyr)
```

The documentation should tell you what the function does, what arguments (i.e., details) needed for it to successfully run, examples, and what the output should look like.

If you know the specific function that you want to look up, you can run this in the R Studio console:

```{r}
??dplyr::filter
```

Once you know what you want to do with the function, you can run it in your code:

```{r}
dat <- # example data frame
    data.frame(stringsAsFactors=FALSE,
               letter = c("A", "A", "A", "B", "B"),
               number = c(1L, 2L, 3L, 4L, 5L))

dat

filter(dat, letter == "A") # using dplyr::filter
```

### Track Two: Welcome to the Tidyverse

The Tidyverse is a set of packages for data manipulation, exploration, and visualization using the design philosophy of 'tidy' data. Tidy data has a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.

The packages contained in the Tidyverse provide useful functions that augment base R functionality.

You can installing and load the complete Tidyverse with:

```{r, eval = FALSE}
install.packages("tidyverse")
```

```{r, message = FALSE}
library(tidyverse)
```

**For more information on tidy data, check out [Hadley Wickhams's Tidy Data paper](http://vita.had.co.nz/papers/tidy-data.html).**

## Loading Data from Various Sources

In this section, we'll load data.

You might be thinking that an Excel file is the first that we would load, but there happens to be a format which you can open and edit in Excel that is even easier to use between Excel and R as well as SPSS and other statistical software, like MPlus, and even other programming languages, like Python. That format is CSV, or a comma-separated-values file. 

The CSV file is useful because you can open it with Excel and save Excel files as CSV files. Additionally, and as its name indicates, a CSV file is rows of a spreadsheet with the columns separated by commas, so you can view it in a text editor, like TextEdit for Macintosh, as well. Not surprisingly, Google Sheets easily converts CSV files into a Sheet, and also easily saves Sheets as CSV files. 

For these reasons, we start with - and emphasize - reading CSV files. 

### Saving a File from the Web

You'll need to copy this URL:

`https://goo.gl/bUeMhV`

Here's what it resolves to (it's a CSV file):

`https://raw.githubusercontent.com/data-edu/data-science-in-education/master/data/pisaUSA15/stu-quest.csv`

This next chunk of code downloads the file to your working directory. Run this to download it so in the next step you can read it into R. As a note: There are ways to read the file directory (from the web) into R. Also, of course, you could do what the next (two) lines of code do manually: Feel free to open the file in your browser and to save it to your computer (you should be able to 'right' or 'control' click the page to save it as a text file with a CSV extension).

```{r, eval = FALSE}
student_responses_url <-
    "https://goo.gl/bUeMhV"

student_responses_file_name <-
    paste0(getwd(), "/data/student-responses-data.csv")

download.file(
    url = student_responses_url,
    destfile = student_responses_file_name)
```

It may take a few seconds to download as it's around 20 MB.

The process above involves many core data science ideas and ideas from programming/coding. We will walk through them step-by-step.

1. The *character string* `"https://goo.gl/wPmujv"` is being saved to an *object* called `student_responses_url`.

```{r}
student_responses_url <-
    "https://goo.gl/bUeMhV"
```

2. We concatenate your working directory file path to the desired file name for the CSV using a *function* called `paste0`. This is stored in another *object* called `student_reponses_file_name`. This creates a file name with a *file path* in your working directory and it saves the file in the folder that you are working in. 

```{r}
student_responses_file_name <-
    paste0(getwd(), "/data/student-responses-data.csv")
```

3. The `student_responses_url` *object* is passed to the `url` argument of the *function* called `download.file()` along with `student_responses_file_name`, which is passed to the `destfile` argument.

In short, the `download.file()` function needs to know
- where the file is coming from (which you tell it through the `url`) argument and
- where the file will be saved (which you tell it through the `destfile` argument).

```{r}
download.file(
    url = student_responses_url,
    destfile = student_responses_file_name)
```

Understanding how R is working in these terms can be helpful for troubleshooting and reaching out for help. It also helps you to use functions that you have never used before because you are familiar with how some functions work.

Now, in R Studio, you should see the downloaded file in the Files tab. This should be the case if you created a project with R Studio; if not, it should be whatever your working directory is set to. If the file is there, great. If things are *not* working, consider downloading the file in the manual way and then move it into the directory that the R Project you created it. 

### Loading a CSV File

Okay, we're ready to go. The easiest way to read a CSV file is with the function `read_csv()` from the package `readr`, which is contained within the Tidyverse.

Let's load the tidyverse library:

```{r, message = F}
library(tidyverse) # so tidyverse packages can be used for analysis
```

You may have noticed the hash symbol after the code that says `library(tidyverse`)`. It reads `# so tidyverse packages can be used for analysis`. That is a comment and the code after it (but not before it) is not run (the code before it runs just like normal). Comments are useful for showing *why* a line of code does what it does. 

After loading the tidyverse packages, we can now load a file. We are going to call the data `student_responses`:

```{r, eval = TRUE}
# readr::write_csv(pisaUSA15::stu_quest, here::here("data", "pisaUSA15", "stu_quest.csv"))
student_responses <-
    read_csv("./data/student-responses-data.csv")
```

Since we loaded the data, we now want to look at it. We can type its name in the function `glimpse()` to print some information on the dataset (this code is not run here).

```{r, eval = FALSE}
glimpse(student_responses)
```

Woah, that's a big data frame (with a lot of variables with confusing names, to boot)!

Great job loading a file and printing it! We are now well on our way to carrying out analysis of our data.

### Loading Excel files

We will now do the same with an Excel file. You might be thinking that you can open the file in Excel and then save it as a CSV. This is generally a good idea. At the same time, sometimes you may need to directly read a file from Excel. Note that, when possible, we recommend the use of CSV files. They work well across platforms and software (i.e., even if you need to load the file with some other software, such as Python).

The package for loading Excel files, `readxl`, is not a part of the tidyverse, so we will have to install it first (remember, we only need to do this once), and then load it using `library(readxl)`. Note that the command to install `readxl` is grayed-out below: The `#` symbol before `install.packages("readxl")` indicates that this line should be treated as a comment and not actually run, like the lines of code that are not grayed-out. It is here just as a reminder that the package needs to be installed if it is not already.

Once we have installed readxl, we have to load it (just like tidyverse):

```{r, eval = FALSE}
install.packages("readxl")
```

```{r}
library(readxl)
```

We can then use the function `read_excel()` in the same way as `read_csv()`, where "path/to/file.xlsx" is where an Excel file you want to load is located (note that this code is not run here):

```{r, eval = FALSE}
my_data <-
    read_excel("path/to/file.xlsx")
```

Of course, were this run, you can replace `my_data` with a name you like. Generally, it's best to use short and easy-to-type names for data as you will be typing and using it a lot. 

Note that one easy way to find the path to a file is to use the "Import Dataset" menu. It is in the Environment window of R Studio. Click on that menu bar option, select the option corresponding to the type of file you are trying to load (e.g., "From Excel"), and then click The "Browse" button beside the File/URL field. Once you click on the, R Studio will automatically generate the file path - and the code to read the file, too - for you. You can copy this code or click Import to load the data.

### Loading SAV files

The same factors that apply to reading Excel files apply to reading `SAV` files (from SPSS). NOte that you can also read CSV file directly into SPSS and so because of this and the benefits of using CSVs (they are simple files that work across platforms and software), we recommend using CSVs when possible. First, install the package `haven`, load it, and the use the function `read_sav()`:

```{r, eval = FALSE}
install.packages("haven")
```

```{r, eval = FALSE}
library(haven)
my_data <-
    read_sav("path/to/file.xlsx")
```

### Google Sheets

Finally, it can sometimes be useful to load a file directly from Google Sheets, and this can be done using the Google Sheets package.

```{r, eval = FALSE}
install.packages("googlesheets")
```

```{r, eval = FALSE}
library(googlesheets)
```

When you run the command below, a link to authenticate with your Google account will open in your browser. 

```{r, eval = FALSE}
my_sheets <- gs_ls()
```

You can then simply use the `gs_title()` function in conjunction with the `gs_read()` function:

```{r, eval = FALSE}
df <- gs_title('title')
df <- gs_read(df)
```

### Saving Files

Using our data frame `student_responses`, we can save it as a CSV (for example) with the following function. The first argument, `student_reponses`, is the name of the object that you want to save. The second argument, `student-responses.csv`, what you want to call the saved dataset.

```{r, eval = FALSE}
write_csv(student_responses, "student-responses.csv")
```

That will save a CSV file entitled `student-responses.csv` in the working directory. If you want to save it to another directory, simply add the file path to the file, i.e. `path/to/student-responses.csv`. To save a file for SPSS, load the haven package and use `write_sav()`. There is not a function to save an Excel file, but you can save as a CSV and directly load it in Excel.

### Conclusion

We will detail the functions used to read every file in a folder (or, to write files to a folder).

## Processing Data

Now that we have loaded `student_responses` into an object, we can process it. This section highlights some common data processing functions. 

We're also going to introduce a powerful, unusual *operator* in R, the pipe. The pipe is this symbol: `%>%`. It lets you *compose* functions. It does this by passing the output of one function to the next. A handy shortcut for writing out `%>%` is Command + Shift + M.

Here's an example. Let's say that we want to select a few variables from the `student_responses` dataset and save those variables into a new object, `student_mot_vars`. Here's how we would do that using `dplyr::select()`.

```{r}
student_mot_vars <- # save object student_mot_vars by...
    student_responses %>% # using dataframe student_responses
    select(SCIEEFF, JOYSCIE, INTBRSCI, EPIST, INSTSCIE) # and selecting only these five variables
```

Note that we saved the output from the `select()` function to `student_mot_vars` but we could also save it back to `student_responses`, which would simply overwrite the original data frame (the following code is not run here):

```{r, eval = FALSE}
student_responses <- # save object student_responses by...
    student_responses %>% # using dataframe student_responses
    select(student_responses, SCIEEFF, JOYSCIE, INTBRSCI, EPIST, INSTSCIE) # and selecting only these five variables
```

We can also rename the variables at the same time we select them. I put these on separate lines so I could add the comment, but you could do this all in the same line, too. It does not make a difference in terms of how `select()` will work.

```{r}
student_mot_vars <- # save object student_mot_vars by...
    student_responses %>% # using dataframe student_responses
    select(student_efficacy = SCIEEFF, # selecting variable SCIEEFF and renaming to student_efficiency
           student_joy = JOYSCIE, # selecting variable JOYSCIE and renaming to student_joy
           student_broad_interest = INTBRSCI, # selecting variable INTBRSCI and renaming to student_broad_interest
           student_epistemic_beliefs = EPIST, # selecting variable EPIST and renaming to student_epistemic_beliefs
           student_instrumental_motivation = INSTSCIE # selecting variable INSTSCIE and renaming to student_instrumental_motivation
    )
```

[will add more on creating new variables, filtering grouping and summarizing, and joining data sets]

## Communicating / sharing results

R Markdown is a highly convenient way to communicate and share results. Navigate to "New File" and then "R Markdown". [add]

Then, click "Knit to PDF", "Knit to HTML", or "Knit to Word".

## Other foundational notes

### Configuring R Studio

There are a number of changes you *can* (but do not need to) make to configure R Studio. If you navigate to the Preferences menu in R Studio, you'll see a number of options you can change, from the appearance of the application to which windows appear where.

One important consideration is whether to save your workspace when you close R Studio. By default, R Studio saves all of the objects in your environment. This means that any data that you have loaded--or new data or objects that you have created, such as by merging two data sets together or creating a plot--will, by default, still exist when you open R Studio next. In general, this is not ideal, because it means that you may have taken steps interactively that are not documented your code. This means that when you share your code, or re-run it from the start, it may not work. An easy way to change this is to tell R Studio to start from scratch (in terms of your workspace) each time you open it. You can do that by changing the dropdown menu pointed out in the image below to "Never".

![optional caption text](images/save-workspace-reminder.jpg)

While this may seem like a dramatic step - never saving your workspace - it is the foundation for doing reproducible work and research using R Studio (and R). It also represents one of the biggest shifts from using software like Excel or SPSS, where most steps are not documented in code. This involves a shift from thinking that your most permanent and important part of an analysis is your data to thinking of the most important part as being the code: with the code, you can keep your data in its original form, process it, and then save a processed file, through running code. This also means that when you have to make a change to this code, you can re-run the entire analysis easily. 

### Getting data in and out

`clipr` is a package to easily copy data into and out of R using the clipboard. [add more]

`datapasta` is another option. [add more]

<!--chapter:end:05-foundational-skills.Rmd-->

---
title: 'Education Dataset Analysis Pipeline: Walkthrough #1'
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = F)
```

# Background 

In the 2015-2016 and 2016-2017 school years, researchers at Michigan State University carried out a study on students' motivation to learn in online science classes. The online science classes were part of a statewide online course provider designed to supplement (and not replace) students' enrollment in their local school. For example, students may choose to enroll in an online physics class because one was not offered at their school (or they were not able to take it given their schedule).

The study involved a number of different data sources which were brought to bear to understand students' motivation:

1. A self-report survey for three distinct but related aspects of students' motivation
2. Log-trace data, such as data output from the learning management system
3. Achievement-related and gradebook data
4. Discussion board data
5. Achievement-related (i.e., final grade) data

First, these different data sources will be described in terms of how they were provided by the school.

## 1. Self-report survey 

This was data collected before the start of the course via self-report survey. The survey included 10 items, each corresponding to one of three *measures*, namely, for interest, utility value, and perceived competence:

1.	I think this course is an interesting subject. (Interest)
2.	What I am learning in this class is relevant to my life. (Utility value)
3.	I consider this topic to be one of my best subjects. (Perceive competence)
4.	I am not interested in this course. (Interest - reverse coded)
5.	I think I will like learning about this topic. (Interest)
6.	I think what we are studying in this course is useful for me to know. (Utility value)
7.	I don’t feel comfortable when it comes to answering questions in this area. (Perceived competence)
8.	I think this subject is interesting. (Interest)
9.	I find the content of this course to be personally meaningful. (Utility value)
10.	I’ve always wanted to learn more about this subject. (Interest)

## 2. Log-trace data 

Log-trace data is data generated from our interactions with digital technologies. In education, an increasingly common source of log-trace data is that generated from interactions with learning management systems. The data for this walk-through is a *summary of* log-trace data, namely, the number of minutes students spent on the course. Thus, while this data is rich, you can imagine even more complex sources of log-trace data (i.e. timestamps associated with when students started and stopped accessing the course!).

## 3. Achievement-related and gradebook data

This is a common source of data, namely, one associated with graded assignments students completed. In this walkthrough, we just examine students' final grade.

## 4. Discussion board data
<!-- NOTE - may not include this, as it is hard to confidently anonymize a medium-ish sized data set -->

Discussion board data is both rich and unstructured, in that it is primarily in the form of written text. We examine a small subset of the discussion board data in this walkthrough.

# Processing the data

```{r, loading-packages}
library(readxl)
library(tidyverse)
library(lubridate)
library(here)
```

```{r}
# Gradebook and log-trace data for F15 and S16 semesters
s12_course_data <- read_csv(
  here(
    "data", 
    "online-science-motivation", 
    "raw", 
    "s12-course-data.csv"
  )
)

# Pre-survey for the F15 and S16 semesters
s12_pre_survey  <- read_csv(
  here(
    "data", 
    "online-science-motivation", 
    "raw", 
    "s12-pre-survey.csv"
  )
) 

# Log-trace data for F15 and S16 semesters - ts is for time spent
s12_time_spent <- read_csv(
  here(
    "data", 
    "online-science-motivation", 
    "raw", 
    "s12-course-minutes.csv"
  )
)
```

## Viewing the data

```{r}
s12_pre_survey 
s12_course_data
s12_time_spent
```

## Processing the pre-survey data

Often, survey data needs to be processed in order to be (most) useful. Here, we process the self-report items into three scales, for: interest, self-efficacy, and utility value. We do this by 

 - Renaming the question variables to something more managable 
 - Reversing the response scales on questions 4 and 7 
 - Categorizing each question into a measure 
 - Computing the mean of each measure 

```{r}
s12_pre_survey  <- s12_pre_survey  %>%
    # Rename the qustions something easier to work with because R is case sensitive
    # and working with variable names in mix case is prone to error
    rename(q1 = Q1MaincellgroupRow1,
           q2 = Q1MaincellgroupRow2,
           q3 = Q1MaincellgroupRow3,
           q4 = Q1MaincellgroupRow4,
           q5 = Q1MaincellgroupRow5,
           q6 = Q1MaincellgroupRow6,
           q7 = Q1MaincellgroupRow7,
           q8 = Q1MaincellgroupRow8,
           q9 = Q1MaincellgroupRow9,
           q10 = Q1MaincellgroupRow10) %>% 
    # Convert all question responses to numeric
    mutate_at(vars(q1:q10), funs(as.numeric))

# Function for reversing scales 
reverse_scale <- function(question) {
    # Reverses the response scales for consistency 
    #   Args: 
    #     question: survey question 
    #   Returns: a numeric converted response
  # Note: even though 3 is not transformed, case_when expects a match for all
  # possible conditions, so it's best practice to label each possible input
  # and use TRUE ~ as the final statement returning NA for unexpected inputs
  x <- case_when(question == 1 ~ 5, 
                 question == 2 ~ 4,
                 question == 4 ~ 2,
                 question == 5 ~ 1,
                 question == 3 ~ 3,
                 TRUE ~ NA_real_)
  x
}

# Reverse scale for questions 4 and 7
s12_pre_survey <- s12_pre_survey %>%
  mutate(q4 = reverse_scale(q4),
         q7 = reverse_scale(q7))

# Add measure variable 
s12_measure_mean <- s12_pre_survey %>% 
    # Gather questions and responses 
    gather(question, response, c(q1:q10)) %>% 
    mutate(
        measure = case_when(
            question %in% c("q1", "q4", "q5", "q8", "q10") ~ "int", 
            question %in% c("q2", "q6", "q9") ~ "uv", 
            question %in% c("q3", "q7") ~ "pc", 
            TRUE ~ NA_character_
        )) %>% 
    group_by(measure) %>% 
    summarise(
        # Mean response for each measure
        mean_response = mean(response, na.rm = TRUE), 
        # Percent of each measure that had NAs in the response field
        percent_NA = mean(is.na(response))
        )

s12_measure_mean
```

## Processing the course data

We also can process the course data in order to obtain more information.

```{r}
# split course section into components
s12_course_data <- s12_course_data %>%
  separate(col = CourseSectionOrigId,
           into = c('subject', 'semester', 'section'),
           sep = '-',
           remove = FALSE)

```

This led to pulling out the subject, semester, and section from the course ID; variables that we can use later on.

## Joining the data

To join the course data and pre-survey data, we need to create similar *keys*. In other words, our goal here is to have one variable that matches across both datasets, so that we can merge the datasets on the basis of that variable. 

For these data, both have variables for the course and the student, though they have different names in each. Our first goal will be to rename two variables in each of our datasets so that they will match. One variable will correspond to the course, and the other will correspond to the student. We are not changing anything in the data itself at this step - instead, we are just cleaning it up so that we can look at the data all in one place.

Let's start with the pre-survey data. We will rename RespondentID and opdata_CourseID to be student_id and course_id, respectively.

```{r}
s12_pre_survey <- s12_pre_survey %>% 
    rename(student_id = RespondentId,
           course_id = opdata_CourseID)

s12_pre_survey
```

Looks better now!

Let's proceed to the course data. Our goal is to rename two variables that correspond to the course and the student so that we can match with the other variables we just created for the pre-survey data.

```{r}
s12_course_data <- s12_course_data %>% 
    rename(student_id = Bb_UserPK,
           course_id = CourseSectionOrigID)

s12_course_data
```

Now that we have two variables that are consistent across both datasets - we have called them "course_id" and "student_id" -  we can join these using the **dplyr** function, `left_join()`. 
Let's save our joined data as a new object called "dat."

```{r}
dat <- left_join(s12_course_data, 
                 s12_pre_survey, 
                 by = c("student_id", "course_id"))

dat
```

Just one more data frame to merge:

```{r}
s12_time_spent <- s12_time_spent %>%
  rename(student_id = Bb_UserPK, 
         course_id = CourseSectionOrigID)
s12_time_spent <- s12_time_spent %>%
  mutate(student_id = as.integer(student_id))
dat <- dat %>% 
  left_join(s12_time_spent, 
            by = c("student_id", "course_id"))
```

Note that they're now combined, even though the course data has many more rows: The pre_survey data has been joined for each student by course combination.

We have a pretty large data frame! Let's take a quick look.

```{r}
dat
```

It looks like we have neary 30,000 observations from 30 variables.

Now that our data are ready to go, we can start to ask some questions of the data. 

# Visualizations and Models

One thing we might be wondering is how time spent on course is related to students' final grade. Let's first calculate the percentage of points students earned as a measure of their final grade (noting that the teacher may have assigned a different grade--or weighted their grades in ways not reflected through the points).

```{r}
dat <- dat %>% 
    group_by(student_id, course_id) %>% 
    mutate(Points_Earned = as.integer(Points_Earned)) %>% 
    summarize(total_points_possible = sum(Points_Possible, na.rm = TRUE),
              total_points_earned = sum(Points_Earned, na.rm = TRUE)) %>% 
    mutate(percentage_earned = total_points_earned/total_points_possible) %>% 
    ungroup() %>% 
    left_join(dat) # note that we join this back to the original data frame to retain all of the variables
```

## Visualization of the relationship between time spent on course and percentage of points earned

<!-- This is really trivial and obvious; need a new/better relationship -->

```{r}
ggplot(dat, aes(x = TimeSpent, y = percentage_earned)) +
    geom_point()
```

There appears to be *some* relationship. What if we added a line of best fit - a linear model?

```{r}
ggplot(dat, aes(x = TimeSpent, y = percentage_earned)) +
    geom_point() + 
    geom_smooth(method = "lm")
```

So, it appeares that the more time students spent on the course, the more points they earned.

# Linear model (regression)

We can find out exactly what the relationship is using a linear model.

```{r}
m_linear <- lm(percentage_earned ~ TimeSpent, data = dat)
summary(m_linear)
```

## But what about different courses?

Is there course-specific differences in how much time students spend on the 
course as well as in how time spent is related to the percentage of points 
students earned?

```{r}
ggplot(dat, aes(x = TimeSpent, y = percentage_earned, color = course_id)) +
    geom_point()
```

```{r}
ggplot(dat, aes(x = TimeSpent, y = percentage_earned, color = course_id)) +
    geom_point() +
    geom_smooth(method = "lm")
```

There appears to be so. One way we can test is to use what is called a multi-level model. This requires a new package; one of the most common for estimating these types of models is **lme4**. We use it very similarly to the `lm()` function, but we pass it an additional argument about what the *groups*, or levels, in the data are.

```{r}
# install.packages("lme4")
library(lme4)
m_course <- lmer(percentage_earned ~ TimeSpent + (1|course_id), data = dat)
summary(m_course)
```

A common way to understand how much variability is at the group level is to calculate the *intra-class* correlation. This value is the proportion of the variability in the outcome (the *y*-variable) that is accounted for solely by the groups identified in the model. There is a useful function in the **sjstats** package for doing this.

```{r}
# install.packages("sjstats")
library(sjstats)
icc(m_course)
```

This shows that nearly 17% of the variability in the percentage of points students earned can be explained simply by knowing what class they are in. 

<!--chapter:end:06-walkthrough-1.Rmd-->

---
title: 'Education Dataset Analysis Pipeline: Walkthrough #2'
output: html_document
---
## Introduction
Gradebooks are nearly ubiquitous throughout K-12 classrooms, whether they exist as standalone Excel files, Google Sheets, or in proprietary software. 

This walkthrough goes through a series of analyses using the data science framework (link), using the sample [Assessment Types - Points](http://web.mit.edu/jabbott/www/excelgradetracker.html) Excel gradebook template from MIT. All data in the sample gradebook have been generated, and do not reflect individual student data.

*** 

## Driving Question and Objectives

***

## Data Import
Setting up our environment (note: how deep do we go into working directories?!)
```{r setup, include=FALSE}
library(tidyverse)
library(here)
```

Importing our data (need to sim data for 25 students)
Check text for object naming conventions, discussion of .csv, .xlsx, versatility of import functions within the `tidyverse`
File naming - issues that can arise from spaces
```{r}
gradebook <- readxl::read_excel(here("/data/gradebooks", "ExcelGradeTrackerAssessmentTypePoints_SIMDATA_01.xlsx"))
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

<!--chapter:end:07-walkthrough-2.Rmd-->

---
title: 'Education Dataset Analysis Pipeline: Walk Through #3'
output: html_document
---

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, eval = TRUE, echo = FALSE, results = 'hide',
                      message = FALSE, warning = FALSE)
```

# Background

One area of interest is the delivery of online instruction, which is becoming more prevalent: in 2007, over 3.9 million U.S. students were enrolled one or more online courses (Allen & Seaman, 2008). 

In this walkthrough, we examine the educational experiences of students in online science courses at a virtual middle school in order to characterize their motivation to achieve and their tangible engagement with the course in terms of behavioral trace measures. To do so, we use a robust data set, which includes self-reported motivation as well as behavioral trace data collected from a learning management system (LMS) to identify predictors of final course grade. Our work examines the idea of educational success in terms of student interactions with an online science course.

One meaningful perspective from which to consider students' engagement with online courses is related to their motivation to achieve. More specifically, it is important to consider how and why students are engaging with the course. Considering the psychological mechanisms behind achievement is valuable because doing so may help to identify meaningful points of intervention for educators and for researchers and administrators in online *and* face-to-face courses interested in the intersection between behavioral trace measures and students' motivational and affective experiences in such courses.

We take up the following four questions:

1. Is motivation more predictive of course grades as compared to other online indicators of engagement?
2. Which types of motivation are most predictive of achievement?
3. Which types of trace measures are most predictive of achievement?
4. How does a random forest compare to a simple linear model (regression)?

# Information about the dataset 

This dataset came from 499 students enrolled in online middle school science courses in 2015-2016. The data were originally collected for use as a part of a research study, though the findings have not been published anywhere, yet.

The setting of this study was a public, provider of individual online courses in a Midwestern state. In particular, the context was two semesters (Fall and Spring) of offerings of five online science courses (Anatomy & Physiology, Forensic Science, Oceanography, Physics, and Biology), with a total of 36 classes. 

Specific information in the dataset included:

- a pre-course survey students completed about their self-reported motivation in science — in particular, their perceived competence, utility value, and interest
- the time students spent on the course (obtained from the LMS, Blackboard) and their final course grades as well as their involvement in discussion forums
- for discussion board responses, we used the Linguistic Inquiry and Word Count (LIWC; Pennebaker, Boyd, Jordan, & Blackburn, 2015) to calculate the number of posts per student and variables for the mean levels of students' cognitive processing, positive affect, negative affect, and social-related discourse
---
title: 'Education Dataset Analysis Pipeline: Walk Through #3'
output: html_document
---

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, eval = TRUE, echo = FALSE, results = 'hide',
                      message = FALSE, warning = FALSE)
```

# Background

One area of interest is the delivery of online instruction, which is becoming more prevalent: in 2007, over 3.9 million U.S. students were enrolled one or more online courses (Allen & Seaman, 2008). 

In this walkthrough, we examine the educational experiences of students in online science courses at a virtual middle school in order to characterize their motivation to achieve and their tangible engagement with the course in terms of behavioral trace measures. To do so, we use a robust data set, which includes self-reported motivation as well as behavioral trace data collected from a learning management system (LMS) to identify predictors of final course grade. Our work examines the idea of educational success in terms of student interactions with an online science course.

One meaningful perspective from which to consider students' engagement with online courses is related to their motivation to achieve. More specifically, it is important to consider how and why students are engaging with the course. Considering the psychological mechanisms behind achievement is valuable because doing so may help to identify meaningful points of intervention for educators and for researchers and administrators in online *and* face-to-face courses interested in the intersection between behavioral trace measures and students' motivational and affective experiences in such courses.

We take up the following four questions:

1. Is motivation more predictive of course grades as compared to other online indicators of engagement?
2. Which types of motivation are most predictive of achievement?
3. Which types of trace measures are most predictive of achievement?
4. How does a random forest compare to a simple linear model (regression)?

# Information about the dataset 

This dataset came from 499 students enrolled in online middle school science courses in 2015-2016. The data were originally collected for use as a part of a research study, though the findings have not been published anywhere, yet.

The setting of this study was a public, provider of individual online courses in a Midwestern state. In particular, the context was two semesters (Fall and Spring) of offerings of five online science courses (Anatomy & Physiology, Forensic Science, Oceanography, Physics, and Biology), with a total of 36 classes. 

Specific information in the dataset included:

- a pre-course survey students completed about their self-reported motivation in science — in particular, their perceived competence, utility value, and interest
- the time students spent on the course (obtained from the LMS, Blackboard) and their final course grades as well as their involvement in discussion forums
- for discussion board responses, we used the Linguistic Inquiry and Word Count (LIWC; Pennebaker, Boyd, Jordan, & Blackburn, 2015) to calculate the number of posts per student and variables for the mean levels of students' cognitive processing, positive affect, negative affect, and social-related discourse

At the beginning of the semester, students were asked to complete the pre-course survey about their perceived competence, utility value, and interest. At the end of the semester, the time students spent on the course, their final course grades, and the contents of the discussion forums were collected.

In this walkthrough, we used the R package **caret** to carry out the analyses.

# Information on random forests

500 trees were grown as part of our random forest. We partitioned the data before conducting the main analysis so that neither the training nor the testing data set would be disproportionately representative of high-achieving or low-achieving students. The training data set consisted of 80% of the original data (n = 400 cases), whereas the testing data set consisted of 20% of the original data (n = 99 cases). We built our random forest model on the training data set, and then evaluated the model on the testing data set. Three variables were tried at each node.

Note that the random forest algorithm does not accept cases with missing data, and so we deleted cases listwise if data were missing. This decision eliminated 51 cases from our original data set, to bring us to our final sample size of 499 unique students.

For our analyses, we used Random Forest modeling (Breiman, 2001). Random forest is an extension of decision tree modeling, whereby a collection of decision trees are simultaneously "grown" and are evaluated based on out-of-sample predictive accuracy (Breiman, 2001).  Random forest is random in two main ways: first, each tree is only allowed to "see" and split on a limited number of predictors instead of all the predictors in the parameter space; second, a random subsample of the data is used to grow each individual tree, such that no individual case is weighted too heavily in the final prediction.

Whereas some machine learning approaches (e.g., boosted trees) would utilize an iterative model-building approach, random forest estimates all the decision trees at once. In this way, each tree is independent of every other tree. Thus, the random forest algorithm provides a robust regression approach that is distinct from other modeling approaches. The final random forest model aggregates the findings across all the separate trees in the forest in order to offer a collection of "most important" variables as well as a percent variance explained for the final model.

A random forest is well suited to the research questions that we had here because it allows for nonlinear modeling. We hypothesized complex relationships between students' motivation, their engagement with the online courses, and their achievement. For this reason, a traditional regressive or structural equation model would have been insufficient to model the parameter space we were interesting in modeling. Our random forest model had one outcome and eleven predictors. A common tuning parameter for machine learning models is the number of variables considered at each split (Kuhn, 2008); we considered three variables at each split for this analysis.  

The outcome was the final course grade that the student earned. The predictor variables included motivation variables (interest value, utility value, and science perceived competence) and trace variables (the amount of time spent in the course, the course name, the number of discussion board posts over the course of the semester, the mean level of cognitive processing evident in discussion board posts, the positive affect evident in discussion board posts, the negative affect evident in discussion board posts, and the social-related discourse evident in their discussion board posts). We used this random forest model to address all three of our research questions.

To interpret our findings, we examined three main things: (1) predictive accuracy of the random forest model, (2) variable importance, and (3) variance explained by the final random forest model.

# Analysis

```{r}
library(tidyverse)
library(caret)
library(here)
```

First, we will load the data, *filter* the data to include only the data from one year, and *select* variables of interest.

```{r}
f <- here::here("online-science-motivation-w-disc.csv")

d <- read_csv(f)

d <-d %>% filter(!str_detect(course_ID, "S217"))

d <- d %>% 
    select(pre_int, pre_uv,  pre_percomp, time_spent,course_ID, final_grade, subject, enrollment_reason, semester, enrollment_status, cogproc, social, posemo, negemo, n)
```

## Use of caret

Here, we remove observations with missing data (per our note above about random forests requiring complete cases).

```{r}
nrow(d)
d <- na.omit(d)
nrow(d)
```

First, machine learning methods often involve using a large number of variables. Oftentimes, some of these variables will not be suitable to use: they may be highly correlated with other variables, for instance, or may have very little - or no - variability. Indeed, for the data set used in this study, one variables has the same (character string) value for all of the observations. We can detect this variable and any others using the following function:

```{r}
nearZeroVar(d, saveMetrics = TRUE)
```

If we look at `enrollment_status`, we will see that it is "Approved/Enrolled" for *all* of the students. When we use this in certian models, t may cause some problems, and so we remove it first.

```{r}
d <- select(d, -enrollment_status)
```

Note that many times you may wish to pre-process the variables, such as by centering or scaling them; we could this with code like the following, which is not run here, as we will first try this out with the variables' original values.

```{r, eval = FALSE}
d <- mutate_if(d, 
               is.numeric, scale)
```

We will want to make character string variables into factors.

```{r}
d <- mutate_if(d, is.character, as.factor)
```

Now, we will prepare the **train** and **test** datasets, using the caret function for creating data partitions. Here, the **p** argument specifies what proportion of the data we want to be in the **training** partition. Note that this function splits the data based upon the outcome, so that the training and test data sets will both have comparable values for the outcome. Note the `times = 1` argument; this function can be used to create *multiple* train and test sets, something we will describe in more detail later.

```{r}
trainIndex <- createDataPartition(d$final_grade,
                                  p = .8, 
                                  list = FALSE,
                                  times = 1)

d_train <- d[ trainIndex,] # can we make this tidy w/ filter
d_test <- d[-trainIndex,]
```

Finally, we will estimate the models.

Here, we will use the train function, passing *all* of the variables in the data frame (except for the outcome, or dependent variable, `final_grade`) as predictors. NOte that you can read more about the specific random forest implementation chosen [here](http://topepo.github.io/caret/train-models-by-tag.html#random-forest).

```{r}
rf_fit <- train(final_grade ~ .,
                data = d_train,
                method = "ranger")

rf_fit
```

We have some results! First, we see that we have 400 samples, or 400 observations, the number in the train data set. No pre-processing steps were specified in the model fitting (note that) these the output of `preProcess` can be passed to `train()` to center, scale, and transform the data in many other ways. Next, note that a resampling technique has been used: this is not for validating the model (per se), but is rather for selecting tuning parameters, or options that need to be specified as a part of the modeling. These parameters can be manually provided, or can be estimated via strategies such as the bootstrap resample (or *k*-folds cross validation).

It appears that the model with the value of the **mtry** tuning parameter equal to 42 seemed to explain the data best, the **splirule* being "extratrees", and **min.node.size** held constant at a value of 5. 

Let's see if we end up with slightly different values if we change the resampling technique to cross-validation, instead of bootstrap resampling.

```{r}
train_control <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 10)

rf_fit1 <- train(final_grade ~ .,
                data = d_train,
                method = "ranger",
                trControl = train_control)

rf_fit1
```

The same tuning parameter values seem to be found with this method. Let's check just one last thing - what if we do not fix **min.node.size** to five?

Let's create our own grid of values to test. We'll stick with the default bootstrap resampling method to choose the best model.

```{r}
tune_grid <- expand.grid(mtry = c(2, 22, 42),
                         splitrule = c("variance", "extratrees"),
                         min.node.size = c(1, 5, 10, 15, 20))

rf_fit2 <- train(final_grade ~ .,
                data = d_train,
                method = "ranger",
                tuneGrid = tune_grid)

rf_fit2
```

The model with the same values as identified before but with **min.node.size** equal to 1 seems to fit best, though the improvement seems to be fairly small relative to the difference the other tuning parameters seem to make. 

Let's take a look at this model. We will first note the large number of independent variables: this is due to the factors being treated as dummy codes. We can also note the *OOB prediction error (MSE)`, of 0.351, and the proportion of the variance explained, or R squared, of 0.658.

```{r}
rf_fit2$finalModel
```

## Predicted values

Using the simpler model without treating `min.node.size` as a tuning paramete

In particular, let's explore predicted values. In particular, we can see how predicted values compare to those in the test set to understand predictive accuracy. 

First, let's calculte the Root Mean Square Error (RMSE), just to gain practice working with the model output.

```{r}
d_train_augmented <- mutate(d_train, 
                  pred = predict(rf_fit, d_train),
                  obs = final_grade, # this is to calculate summmary statistics using caret
                  raw_diff = pred - final_grade,
                  abs_diff = abs(pred - final_grade))

d_train_augmented %>%
    summarize(MAE = mean(abs_diff),
              RMSE = sqrt(mean(raw_diff^2)))
```

We can calculate these automatically using the **caret** `defaultSummary()` function (which just requires columns with `obs` and `pred` in it in a data frame):

```{r}
# install.packages("MLmetrics")
defaultSummary(as.data.frame(d_train))
```

The RMSE and MAE values correspond to those we calculated manually. Note that an $R^2$ value is also calculated as the square of the correlation between the observed and predicted values. 

So, what do these values tell us? On average, our predictions are around 4 percentage points away from their actual values. Not so bad!

Why use RMSE, though, over MAE? [Note: don't kow why].

# Examining predictive accuracy on the test data set

What if we use the test data set - data not used to train the model?

```{r, eval = FALSE}
# NOT RUN AS I WAS HAVING TROUBLE GETTING THIS TO WORK BEACUSE OF A FACTOR LEVEL NOT IN D_TEST - we've had this issue before and there are workarounds
d_test_augmented <- mutate(d_test,
                           pred = predict(rf_fit, d_test),
                           obs = final_grade)

defaultSummary(as.data.frame(d_test_augmented))
```

We can compare this to the values above to see how much poorer - if at all - performance is on data not used to train the model.

## Variable importance measures

We can examine two different variable importance measures using the **ranger** method in **caret**.

Note that importance values are not calcultaed automatically, but that "impurity" or "permutation" can be passed to the `importance` argument in `train()`. See more [here](https://alexisperrier.com/datascience/2015/08/27/feature-importance-random-forests-gini-accuracy.html).

We'll re-run the model, but will add an argument to call the variable importance metric.

```{r}
rf_fit_imp <- train(final_grade ~ .,
                data = d_train,
                method = "ranger",
                importance = "impurity")

varImp(rf_fit_imp)
```

We can visualize these:

```{r}
varImp(rf_fit_imp) %>%
    pluck(1) %>%
    rownames_to_column("var") %>%
    ggplot(aes(x = reorder(var, Overall), y = Overall)) +
    geom_col() +
    coord_flip()
```

We can see whether these change with the different importance measures.

```{r}
rf_fit_imp_permutation <- train(final_grade ~ .,
                data = d_train,
                method = "ranger",
                importance = "permutation")

varImp(rf_fit_imp_permutation) %>%
    pluck(1) %>%
    rownames_to_column("var") %>%
    ggplot(aes(x = reorder(var, Overall), y = Overall)) +
    geom_col() +
    coord_flip()
```

They are similar but somewhat different. One takeaway from this analysis is that what course students are in seems to have a different effect depending on the course. Also, how much students write in their discussion posts (`n`) seems to be very important - as does the time students spend in their course. Finally, there are some subject level differences (in terms of how predictive subject is). Perhaps grades should be normalized within subject: would this still be an important predictor, then?

## Comparing a random forest to a regression

You may be curious about comparing the predictive accuracy of the model to a linear model (a regression).

```{r, warning = FALSE}
d_train_lm <- mutate_if(d_train, is.character, as.factor) # may want to move this earlier; it seems important for method = lm but not ranger, for some reason

lm_fit <- train(final_grade ~ .,
                data = d_train_lm, 
                method = "lm")

d_train_lm <- mutate(d_train, 
                     obs = final_grade,
                     pred = predict(lm_fit, d_train_lm))

defaultSummary(as.data.frame(d_train_lm))
defaultSummary(as.data.frame(d_train_augmented))
```

We can see that the random forest technique seems to perform better than regression. It may be interesting to compare the results from the random forest not to a more straightforward model, such as a regression, but to a more sophisticated model, like one for deep learning. For now, we'll leave that to you.

<!--chapter:end:08-walkthrough-3.Rmd-->

---
title: 'Education Dataset Analysis Pipeline: Walkthrough #4'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, cache=FALSE)
here::set_here(path = "~/documents/data-science-in-education")
```

## Background

Relationships are important to us. In the case of many research techniques, relationships are&mdash;deservedly&mdash;the focus of analyses. It is not very difficult to imagine *qualitative* techniques to study relationships: one could ask other individuals about who their friends are, why they are their friends, and what they like to do when with them. 

Increasingly, it is also not hard to imagine *quantitative* techniques to study relationships, too. In a way, the same questions that could be used qualitatively can serve as the basis for the quantitative study of relationships. Indeed, **social network analysis** uses these relations in a range of visualizations as well as statistical models.

## Creating a network graph

Now, we'll use the **igraph** package to create a *graph* of our simulated data's network. Once we have this graph, we can calculate many useful descriptive statistics associated with the network's features:  

- *Diameter*: The length of the longest geodesic; that is, the max distance between two nodes in the network.  
- *Density*: The ratio of the number of edges and the number of possible edges for a network of that size.  
- *Transitivity*: The balance of connections. Also called the clustering coefficient. The probability that the adjacent vertices of a vertex are connected. When the clustering coefficient is large it implies that a graph is highly clustered around a few nodes; when it is low it implies that the links in the graph are relatively evenly spread among all the nodes (Hogan, 2017).  
- *Reciprocity*: The proportion of mutual connections (in a directed network). The probability that the opposite counterpart of a directed edge is also included in the graph.  
- *Degree*: The number of connections someone has with others nominating or being a nominee (Kadushin, 2012).

Keep in mind that measures such as diameter and density “can be misleading when comparing graphs of substantially different sizes” (Hogan, 2017, p. 255). Therefore, these measures should only be used to compare networks of similar size, or the same network at different points in time.

First, let's create the network graph, called `sim_graph`:

```{r}
library(igraph)
sim_graph <- data %>% select(nominator, nominee) %>%  # this creates an edgelist
    as.matrix %>% 
    graph_from_edgelist(directed=TRUE) %>%
    set_vertex_attr(name='degree', value=degree(., mode='total', loops=FALSE)) %>% 
    set_vertex_attr(name='in_degree', value=degree(., mode='in', loops=FALSE)) %>% 
    set_vertex_attr(name='out_degree', value=degree(., mode='out', loops=FALSE))

network_summary <- sim_graph %>% V %>% length %>%  # number of vertices/nodes
    rbind(sim_graph %>% gsize) %>%  # number of edges
    rbind(sim_graph %>% diameter) %>%  # max distance between two vertices
    rbind({sim_graph %>% edge_density * 100} %>% round(2)) %>%     
    rbind({sim_graph %>% transitivity("global") * 100} %>% round(2)) %>% 
    rbind({sim_graph %>% reciprocity * 100} %>% round(2))  %>%
    rbind(sim_graph %>% vertex_attr('degree') %>% mean %>% round(2)) %>% 
    rbind(sim_graph %>% vertex_attr('degree') %>% sd %>% round(2)) %>% 
    rbind(sim_graph %>% vertex_attr('degree') %>% median) %>% 
    rbind(sim_graph %>% vertex_attr('degree') %>% min) %>%
    rbind(sim_graph %>% vertex_attr('degree') %>% max)

colnames(network_summary) <- c("")
rownames(network_summary) <- c("Number of nodes: ", "Number of edges: ", "Diameter: ",
                               "Density: ", "Transitivity: ", "Reciprocity: ",
                               "Mean degree: ", "SD degree: ", "Median degree: ",
                               "Min degree: ", "Max degree: ")
network_summary
```

## Clustering

With a graph of our simulated network, we can see if *clustering* occurs in this network and describe some characteristics of these clusters.  

First, a definition: a *cluster*&mdash;also called a community or group&mdash;is a set of nodes with many edges inside the community and few edges between outside it (i.e. between the community itself and the rest of the graph). There are numerous methods for determining network clusters, but here we use the *spinglass clustering algorithm*, which maps community detection onto finding the ground state of an infinite range spin glass. Csardi, Nepusz, and Airoldi (2016, pp. 132-133) explained:

>
The clustering method of [Reichardt and Bornholdt](https://arxiv.org/abs/cond-mat/0603718) (2006) is motivated by spin glass models from statistical physics. Such models are used to describe and explain magnetism at the microscopic scale at finite temperatures. Reichardt and Bornholdt (2006) drew an analogy between spin glass models and the problem of community detection on graphs and proposed an algorithm based on the simulated annealing of the spin glass model to obtain well-defined communities in a graph. A spin glass model consists of a set of particles called spins that are coupled by ferromagnetic or antiferromagnetic bonds. Each spin can be in one of k possible states. The well-known Potts model then defines the total energy of the spin glass with a given spin configuration... Spins and interactions in the Potts model are very similar to graphs: each spin in the model corresponds to a vertex, and each interaction corresponds to an edge... Reichardt and Bornholdt (2006) gave efficient update rules for the above energy function, making it possible to apply a simulated annealing procedure to find the ground state of the model that corresponds to a low energy configuration. Their algorithm starts from a random configuration of spins and tries to flip all the spins once in each time step. After each individual spin flip, the energy of the new configuration is evaluated.
>

In other words, the spinglass clustering algorithm partitions nodes into communities by optimizing an energy function. The energy is optimized using the following function (Reichardt and Bornholdt, 2008): 
$$H({\sigma}) = -\sum(a_{ij} \textrm{internal links}) + \sum(b_{ij}\textrm{internal non-links}) + \sum(c_{ij}\textrm{external links}) - \sum(d_{ij}\textrm{external non-links})$$. 

This function penalizes missing edges or non-links of people/nodes in the same cluster and present links or edges between people/nodes in different clusters. It also rewards present links or edges between people/nodes in the same cluster and missing links or edges between people/nodes in different clusters. Thus, a lower score (i.e., lower energy level) is better as it means that the internal links and external non-links have more weightage in that model. In other words, in a strong model, members within clusters are strongly linked and members in separate clusters are weakly linked . Here, $a_{ij}, b_{ij}, c_{ij}, d_{ij}$ represent the individual weights of the four components. 

The initial R code to produce spinglass clusters is straightforward. First, we identify one "giant" cluster&mdash;basically, all nodes that have even a loose connection to each other and create a new network graph by removing any nodes that are not part of the giant cluster. Let's call the graph of the giant cluster `giant_cluster`:

```{r}
library(igraph)

giant_cluster <- sim_graph %>% 
    set_vertex_attr(name='membership', 
                    value = clusters(sim_graph) %>% as.data.frame %>% pull(membership)
    ) %>%
    delete_vertices({vertex_attr(., 'membership') != 1} %>% which)
```

Next, we separate the giant cluster into more meaningful clusters, as determined by the spinglass clustering algorithm. We store the cluster information in `csg0`:

```{r}
<<<<<<< HEAD
library(tidyverse)

create_yvar2 = function(yv1) {
  # Creates yvar2 as a linear outcome of yvar1 
  # Args: 
  #   yv1: yvar1
  yv1 * .25 + rnorm(n = 1, mean = 10, sd = 100)
}

#------------------------------------------------------------------------------

# Make the dataset 
high_y_values <- tibble(
  yvar1 = sample(1000:10000, 100, replace = TRUE)) %>% 
  mutate(yvar2 = map_dbl(yvar1, create_yvar2))

```

```{r}
library(simstudy)
=======
csg_0 <- giant_cluster %>% cluster_spinglass  # creates the clusters; 'csg' = cluster spinglass
csg_0$membership %>% unique %>% length  # number of clusters/communities/groups
```
>>>>>>> 58d1f3d32df76c709efd19069c532b125f465a3d

One of the important outcomes of this method is the _modularity_ value $M$. Modularity measures how good the division is, or how separated are the different vertex types from each other. The spinglass algorithm looks for the modularity of the optimal partition. For a given network, the partition with maximum modularity corresponds to the optimal community structure (i.e., a higher $M$ is better).

The maximum modularity score is +1 and according to Hogan (2017), networks with a modularity above 0.3 as "very modular," meanig that most edges are within communities. Note also that if $M$ = 0, all nodes belong to one group.

<<<<<<< HEAD
data1 <- genData(500, def)
data1
=======
```{r modularity, include=TRUE}
csg_0$modularity
>>>>>>> 58d1f3d32df76c709efd19069c532b125f465a3d
```

### Identifying the "typical" number of clusters returned with the spinglass algorithm

It is important to note that a different result is returned each time the spinglass clustering algorithm is run. For this reason, we needed to run a number of simulations to see what the "typical" number of clusters are. We ran the algorithm 100 times and looked at the mean and median number of clusters obtained. We made a note of a _seed_ that produced the median number of clusters, confirmed that this was reproducible, and then set this seed so that all future work will be run with this same clustering configuration.

```{r}
csg_matrix <- matrix(NA, nrow=1, ncol=100)
for (i in 1:100) {
    print(i)
    set.seed(i)
    csg = giant_cluster %>% cluster_spinglass
    csg_matrix[1,i] <- max(csg$membership)
}

csg_matrix_summary <- csg_matrix %>% length %>%
    rbind(csg_matrix %>% mean %>% round(2)) %>% 
    rbind(csg_matrix %>% sd %>% round(2)) %>% 
    rbind(csg_matrix %>% median) %>% 
    rbind(csg_matrix %>% min) %>%
    rbind(csg_matrix %>% max)
colnames(csg_matrix_summary) <- c("")
rownames(csg_matrix_summary) <- c("number of tests: ", "mean: ", "sd: ",  "median: ", "min: ", "max: ")
csg_matrix_summary
```

```{r}
## select a seed from this list which reproduces the median number of clusters
seeds <-{as.vector(csg_matrix) == median(csg_matrix)} %>% which
our_seed <- seeds[1]
set.seed(our_seed)  # set the seed
csg <- giant_cluster %>% cluster_spinglass

csg_summary <- csg$vcount %>% 
    rbind(giant_cluster %>% gsize) %>% 
    rbind(csg$csize %>% length) %>% 
    rbind(csg$modularity %>% round(4))
colnames(csg_summary) <- c("")
rownames(csg_summary) <- c("Number of nodes: ", "Number of edges: ", "Number of clusters: ", "Modularity: ")
csg_summary

print("Size of each cluster: ", quote=FALSE); print(csg$csize)
```

### Test of statistical significance for spinglass clusters

The test for statistical significance for spinglass clustering is a bit different than the familiar tests that return $p$-values (Csardi, Nepusz, & Airoldi (2016, pp. 132-138).

The idea behind this test of significance is that a random network of equal size and degree distribution as our observed network should have a lower modularity score--that is, if the observed network does in fact have statistically significant clustering.

The following R procedure generates 100 randomized instances of our network (with the same size and degree distribution) using the `sample_ degseq()` function. The `method = 'vl'` ensures that there are no loop edges in the randomly generated networks. We then applied the spinglass clustering algorithm to each of the 100 randomized instances of the network.

A '0' result from this procudure indicates that no randomized networks have community structure with a modularity score that is higher than the one obtained from the original, observed network. Hence a '0' result means that our network has significant community structure; any non-zero results means that the detected spinglass clusters are not statistically significant.

```{r spinglass_sig_test, include=TRUE}
degrees <- giant_cluster %>% as.undirected %>% degree(mode='all', loops=FALSE)
qr_vl <- replicate(100, sample_degseq(degrees, method="vl"), 
                   simplify=FALSE) %>%
    lapply(cluster_spinglass) %>%
    sapply(modularity) 
sum(qr_vl > csg$modularity) / 100
```

## Network Visualization with Clusters

Visualizations of social networks are interesting and powerful--and increasingly common.

Here, we create a visualization of our network structure, using the color palette generated by our spinglass clustering.

```{r}
## color-blind palette
## source: https://jacksonlab.agronomy.wisc.edu/2016/05/23/15-level-colorblind-friendly-palette/
palette <- c("#000000","#004949","#009292","#ff6db6","#ffb6db",
             "#490092","#006ddb","#b66dff","#6db6ff","#b6dbff",
             "#920000","#924900","#db6d00","#24ff24","#ffff6d")
csg_palette <- palette[csg$membership]
```

Here, we used the _Fruchterman-Reingold layout algorithm_ (`layout = 'fr'`), which is appropriate for large (but still with less than 1,000 nodes), potentially disconnected networks.

```{r}
library(ggraph)
library(gridExtra)

layout_randomly <- giant_cluster %>% create_layout(layout='randomly')
layout_mds <- giant_cluster %>% create_layout(layout='mds')
layout_fr <- giant_cluster %>% create_layout(layout='fr')
layout_drl <- giant_cluster %>% create_layout(layout='drl')
layout_kk <- giant_cluster %>% create_layout(layout='kk')
layout_sugiyama <- giant_cluster %>% create_layout(layout='sugiyama')

## Additional layout algorithms to try:
#layout_auto <- giant_cluster %>% create_layout(layout='nicely')
#layout_lgl <- giant_cluster %>% create_layout(layout='lgl')
#layout_dh <- giant_cluster %>% create_layout(layout='dh')
#layout_graphopt <- giant_cluster %>% create_layout(layout='graphopt')

viz_random <- ggraph(layout_randomly) +
    geom_edge_link(width=.1, arrow = arrow(length=unit(1, 'mm'))) +
    geom_node_point(alpha=.75, size=4, color=csg_palette) +
    theme_bw() +
    theme(plot.background = element_blank(), panel.border = element_blank(),
          panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          axis.title = element_blank(), axis.text = element_blank(),
          axis.ticks = element_blank(), legend.position="none"
    )
viz_mds <- ggraph(layout_mds) +
    geom_edge_link(width=.1, arrow = arrow(length=unit(1, 'mm'))) +
    geom_node_point(alpha=.75, size=4, color=csg_palette) +
    theme_bw() +
    theme(plot.background = element_blank(), panel.border = element_blank(),
          panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          axis.title = element_blank(), axis.text = element_blank(),
          axis.ticks = element_blank(), legend.position="none"
    )
viz_fr <- ggraph(layout_fr) +
    geom_edge_link(width=.1, arrow = arrow(length=unit(1, 'mm'))) +
    geom_node_point(alpha=.75, size=4, color=csg_palette) +
    theme_bw() +
    theme(plot.background = element_blank(), panel.border = element_blank(),
          panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          axis.title = element_blank(), axis.text = element_blank(),
          axis.ticks = element_blank(), legend.position="none"
    )
viz_drl <- ggraph(layout_drl) +
    geom_edge_link(width=.1, arrow = arrow(length=unit(1, 'mm'))) +
    geom_node_point(alpha=.75, size=4, color=csg_palette) +
    theme_bw() +
    theme(plot.background = element_blank(), panel.border = element_blank(),
          panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          axis.title = element_blank(), axis.text = element_blank(),
          axis.ticks = element_blank(), legend.position="none"
    )
viz_kk <- ggraph(layout_kk) +
    geom_edge_link(width=.1, arrow = arrow(length=unit(1, 'mm'))) +
    geom_node_point(alpha=.75, size=4, color=csg_palette) +
    theme_bw() +
    theme(plot.background = element_blank(), panel.border = element_blank(),
          panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          axis.title = element_blank(), axis.text = element_blank(),
          axis.ticks = element_blank(), legend.position="none"
    )
viz_sugiyama <- ggraph(layout_sugiyama) +
    geom_edge_link(width=.1, arrow = arrow(length=unit(1, 'mm'))) +
    geom_node_point(alpha=.75, size=4, color=csg_palette) +
    theme_bw() +
    theme(plot.background = element_blank(), panel.border = element_blank(),
          panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          axis.title = element_blank(), axis.text = element_blank(),
          axis.ticks = element_blank(), legend.position="none"
    )

grid.arrange(viz_random, viz_mds, viz_fr,
             viz_drl, viz_kk, viz_sugiyama, 
             nrow = 3)
```

<<<<<<< HEAD
=======
The layouts above are as follows: the first row has 1.) a *random* plotting of nodes and 2.) the *MDS* algorithm. The second row has 3.) the *FR* algorithm and 4.) the *DRL* algorithm. The third row as 5.) the *KK* algorithm, and 6.) the *Sugiyama* algorithm. Each of these is appropriate in different situations; if unsure of which to use, the *nicely* option automatically selects an algorithm based on the network size.

## Selection and influence

Behind these visualizations, though, there are also statistical models and methods that can help to understand what is going on with respect to particular relationships in a network in additional ways.

One way to consider these models and methods is in terms of two *processes* at play in our relationships (cite). These two processes are commonly (though not exclusively) the focus of statistical analyses of networks. In addition to not being exclusive, they do not interact independently: they affect each other reciprocally (Xu, Frank, & Penuel, 2018). They are:

- *Selection*: the processes regarding who chooses to have a relationship with whom
- *Infuence*: the processes regarding how who we have relationships with affects our behavior

While these are complex, they can be studied with the type of data collected from asking people about their relationships (and possibly asking them about or studying their behavior--or measuring some outcome). Happily, the use of these methods has expanded along with **R**: many of the best tools for studying social networks are in the form of long-standing R packages. Additionally, while there are many potential naunces to studying selection and influence, these are models that can fundamentally be carried out with regression, or the linear model (or extensions of it).

In this walkthrough, the influence model is the focus. Nevertheless, we provide some direction for how to carry out selection modeling, too, at the end. 

## Creating example data in the form of an edgelist 

First, let's create three different data frames. Here is what they should contain:

- A data frame indicating who the *nominator* and *nominee* for the relation (i.e., if Stefanie says that José is her friend, then Stefanie is the nominator and José the nominee) - as well as an optional variable indicating the weight, or strength, of their relation.
- This data frame and its type can be considered the basis for many types of social network analysis and is a common structure for network data: it is an *edgelist*.
- Data frames indicating the values of some behavior - an outcome - at two different time points.

## An example of influence

In this example, we create some example data that can be used to explore questions about how influence works. Note that Joshua Rosenberg and Sarah Galey initially wrote the following code for a walkthrough shared on Ken Frank's website [here](https://msu.edu/~kenfrank/resources.htm).

>>>>>>> 58d1f3d32df76c709efd19069c532b125f465a3d
Let's take a look at the merged data. What this data now contains is the first data frame, `data1`, with each nominees' outcome at time 1 (`yvar1`). Note that we will find each nominators' outcome at time 2 later on.

```{r}
data <- as_tibble(data)
data
```

### Calculating an exposure term

This is the key step that makes this model - a regression, or linear, model - one that is special. It is creating an exposure term. The idea is that the exposure term "captures" how your interactions with someone, over some period of time (between the first and second time points) impact some outcome. This model accounts for an individual's initial report of the outcome, i.e., their time 1 prior value, so it is a model for *change* in some outcome.

```{r}
# Calculating exposure
data$exposure <- data$relate * data$yvar1

# Calculating mean exposure
mean_exposure <- data %>%
    group_by(nominator) %>%
    summarize(exposure_mean = mean(exposure))
<<<<<<< HEAD

=======
>>>>>>> 58d1f3d32df76c709efd19069c532b125f465a3d
```

What this data frame - `mean_exposure` - contains is the mean of the outcome (in this case, `yvar1`) for all of the individuals the nominator had a relation with.

As we need a final data set with `mean_exposure`, `mean_exposure_plus`, `degree`, `yvar1`, and `yvar2` added, we'll process the data a bit more.

```{r}
mean_exposure_terms <- left_join(mean_exposure, mean_exposure_plus, by = "nominator")

names(data2) <- c("nominator", "yvar1") # rename nominee as nominator to merge these
final_data <- left_join(mean_exposure_terms, data2, by = "nominator")
final_data <- left_join(final_data, data3, by = "nominator") # data3 already has nominator, so no need to change
```

### Regression (linear models)

Calculating the exposure term is the most distinctive and important step in carrying out influence models. Now, we can simply use a linear model to find out how much relations - as captured by the influence term - affect some outcome.

```{r}
model1 <- lm(yvar2 ~ yvar1 + exposure_mean, data = final_data)
summary(model1)
```

Note that these models show ... [add]

So, the influence model is used to study a key process for social network analysis, but it is one that is useful, because you can quantify, given what you measure and how you measure it, *the network effect*, something that is sometimes not considered, especially in education (Sweet, 2017). It's also fundamentally a regression. That's really it, as the majority of the work goes into calculating the exposure term.

## Selection models

While this tutorial focused on influence models, selection models are also commonly used - and are commonly of interest not only to researchers but also to administrators and teachers (and even to youth and students). 

Here, we briefly describe a few possible approaches for using a selection model.

At its core, the selection model is a regression - albeit, one that is a generalization of one, namely, a logistic regression (sometimes termed a generalized linear model, because it is *basically* a regression but is one with an outcome that consists just of 0's and 1's). Thus, the most straight-away way to use a selection model is to use a logistic regression where all of the relations (note the `relate` variable in `data1` above) are indicated with a 1. But, here is the important and challenging step: all of the *possible relations* (i.e., all of the relations that are possible between all of the individuals in a network) are indicated with a 0 in an edgelist. Note that, again, an edgelist is the preferred data structure for carrying out this analysis. This step involves some data wrangling, especially the idea of widening or lengthening a data frame.

<!-- May want to add a short bit of code on this using `gather()` and `spread()` -->

Once all of the relations are indicated with a 1 or a 0, then a simple linear regression can be used. Imagine that we are interested in whether individuals from the *same* group are more or less likely to interact than those from different groups; same could be created in the data frame based upon knowing which group both nominator and nominee are from:

```{r, eval = FALSE}
m_selection <- glm(relate ~ 1 + same, data = edgelist1)
```

While this is a straightforward way to carry out a selection model, there are some limitations to it. Namely, it does not account for individuals who send more (or less) nominations overall--and not considering this may mean other effects, like the one associated with being from the *same* group, are not accurate. A few extensions of the linear model - including those that can use data for which relationships are indicated with weights, not just 1's and 0's, have been developed. 

One type of model extends the logistic regression. It can be used for data that is not only 1's and 0's but also data that is normally distributed or has fixed-ranks. It is the **amen** package available [here](https://cran.r-project.org/web/packages/amen/index.html).

A particularly common one is an Exponential Random Graph Model, or an ERGM. An R package that makes estimating these easy is available [here](https://cran.r-project.org/web/packages/ergm/index.html). That R package, **ergm**, is part of a powerful and often-used collection of packages, including those for working with network data (data that can begin with an edgelist, but may need additional processing that is challenging to do with edgelist data), **statnet**. A link to the statnet packages is [here](https://statnet.org/).

## References

Bates, D., Maechler, M., Bolker, B., & Walker, S. (2018). lme4: Linear mixed-effects models using 'Eigen' and S4 (Version 1.1-19) [R package]. Retrieved from https://cran.r-project.org/package=lme4

Csardi, G. (2018). igraph: Network analysis and visualization (Version 1.2.2) [R package]. Retrieved from https://CRAN.R-project.org/package=igraph

Csardi, G., Nepusz, T., & Airoldi, E. M. (2016). *Statistical network analysis with igraph*. New York, NY: Springer. Retrieved from https://sites.fas.harvard.edu/~airoldi/pub/books/BookDraft-CsardiNepuszAiroldi2016.pdf

Hogan, B. (2017). Online social networks: Concepts for data collection and analysis. In N. G. Fielding, R. M. Lee, & G. Blank (Eds.), *The SAGE handbook of online research methods* (2nd ed., pp. ). London, UK: SAGE.

Kadushin, C. (2012). *Understanding social networks: Theories, concepts, and findings.* New York, NY: Oxford University Press.

Pedersen, T. L. (2018). ggraph: An implementation of grammar of graphics for graphs and networks (Version 1.0.2) [R package]. Retrieved from https://CRAN.R-project.org/package=ggraph

R Core Team. (2018). R: A language and environment for statistical computing (Version 3.5.0) [Computer software]. Vienna, Austria: R Foundation for Statistical Computing. Retrieved from https://www.R-project.org/

Reichardt, J., & Bornholdt, S. (2006). Statistical mechanics of community detection. *Physical Review E, 74*(1), 016110. Retrieved from https://arxiv.org/abs/cond-mat/0603718

Wickham, H., Chang, W., & RStudio. (2016). ggplot2: Create elegant data visualisations using the grammar of graphics (Version 2.2.1) [R package]. Retrieved from https://CRAN.R-project.org/package=ggplot2

Wickham, H., Francois, R., Henry, L., Muller, K., & RStudio. (2018). dplyr: A grammar of data manipulation (Version 0.7.6) [R package]. Retrieved from https://CRAN.R-project.org/package=dplyr

<!--chapter:end:09-walkthrough-4.Rmd-->

---
title: ""
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = F)
```

# Background

A common situation encountered when using data for analyzing the education sector, particularly for analysts who are not directly working with schools or districts, is the prevalence of aggregate data. [Aggregate data refers to numerical or non-numerical information that is (1) collected from multiple sources and/or on multiple measures, variables, or individuals and (2) compiled into data summaries or summary reports, typically for the purposes of public reporting or statistical analysis.](https://www.edglossary.org/aggregate-data/) Example of publicly available aggregate data include school-level graduation rates or state test proficiency scores by grade and subject.

These datasets are large, lagging, and often suppressed to protect privacy. Because of their coarseness, they can be difficult to use in decision-making. However, these datasets are available to analysts for gleaning insights about education and can be used for landscape analyses or to supplement other analyses.

# The Data

### Understanding Trends

### Making Comparisons

### Finding Inequities













<!--chapter:end:10-walkthrough-5.Rmd-->

# Advanced Uses  

## Multi-level models 

This is an example from the online science motivation dataset.

```{r}
d <- readr::read_csv("data/online-science-motivation.csv")
d
```

## Text analysis

## Longitudinal analysis  

<!--chapter:end:11-advanced-uses.Rmd-->

# Solutions for Adopting Data Science Techniques in Education

<!--chapter:end:12-solutions.Rmd-->

# Students doing data science

<!--chapter:end:13-students-doing-data-science.Rmd-->

# Resources used in the creation of this text

__[National Center for Research in Policy and Practice: Findings from a National Study on Research Use Among School and District Leaders](http://ncrpp.org/assets/documents/NCRPP_Technical-Report-1_National-Survey-of-Research-Use.pdf)__   

_Summary_: A survey of 733 school principals and district leaders within US mid-sized and large school districts, focused on how educational leaders use research to inform their decision-making.

<!--chapter:end:14-resources.Rmd-->

# Learning More

While this book aims to serve as an introduction to data science in education,
there is much more to learn.

## How to learn more: Work in the open

Sharing one's work in the open is a great way to learn more. We discuss how
welcoming others, sharing one's work, asking for and providing help, and taking
a growth mindset are ways to learn more

### Welcoming others

One strength of the data science community (at least the corner of it that the
authors of this book occupy) is that it can be a welcoming place. To the extent
that the data science community is this - a welcoming place - it is due to the
hard work of many of the people in the data science community. In order for the
data science community to continue to be welcoming - and to be more welcoming to
those for whom its currently not - it's important for all of us to continue to
build a data science community that is inclusive of those who wish to be
involved. Especially to those of us in education, there are many opportunities
to welcome others. One thing to be aware of: You may end up liking data science
so much that you become an evangelist (and may, if you are like us, on occasion,
make people think you are a bit crazy for R!). This is one thing to keep in
mind as regards welcoming others: People will want to use data science at
different times and for different reasons. Keeping this in mind can keep one's
focus on welcoming those who are already interested and ready to learn more.
Welcoming others has the side benefit of positioning you as someone with (even
if it is still-developing) expertise in data science and R. Teaching others - or
even helping others to get started - is a great way to learn more about one's
understanding and to learn more in the process.

## Sharing one's work

A great way to learn more is to share one's work in the open. The first concern
that many of us have about sharing our work in the open is that it is imperfect.
This is a dilemma that everyone manages differently; some data scientist's blog
posts are very polished and article-like, whereas others are comfortable sharing
ideas-in-progress or shorter posts. Especially for sharing work related to data
science, there are blogging platforms that make it easy to publish code, its
output, and text. Hill has a great introduction to getting up and running with
Blogdown [here](https://alison.rbind.io/post/up-and-running-with-blogdown/).

## Asking for and receiving help

There are a number of platforms for asking for and receiving help that can be a
great place to learn more. Questions and answers from sites such as [R Studio
Community](https://community.rstudio.com/) and [Stack
Overflow](https://stackoverflow.com/) are often among the first items that are
returned from a search on a topic. Asking one's own question on these platforms
is a great way to get help specific to the problem you are facing; one helpful
strategy when asking a question is to include a *reprex*, a reproducible
example. See Bryan's page with resources related to reprexes
[here](https://community.rstudio.com/t/video-reproducible-examples-and-the-reprex-package/14732).
Of course, answering questions is also a great way to help others - and to learn
more about the challenges others face or how to approach specific problems in
the process.

### Adopting a growth mindset

Doing data science and using R are hard. Moreover, both are changing rapidly and
it can feel difficult to keep up.

We suggest taking a *growth mindset* to learning data science. In Dweck's
characterization, when it comes to our capabilities, we can think of ourselves
as *being* (and *not being*) mathematics or reading people, with this being an
unchangeable part of who we are. Conversely, we can think of ourselves as being
more or less capable depending on how much we want to be, who we are around, and
how much effort we exert. The former way of thinking is a *fixed* mindset,
whereas the latter is a *growth* mindset. As educators, we would be remiss not
to mention some of the critiques (and concerns) about focusing exclusively on a
growth mindset (see this post
[here](https://www.edweek.org/ew/articles/2015/09/23/carol-dweck-revisits-the-growth-mindset.html)),
but it can be a useful idea when used narrowly and as a spark to push further
and to learn more.

We would like to suggest that having a growth mindset is especially helpful for
learning more about data science. Partially, this is because what data science
is is still being discussed: and so nobody is born a data scientist! More
importantly, individuals come to data science - and make contributions to data
science - with many different backgrounds, many of them unrelated to STEM
(Science, Technology, Engineering, and Mathematics) backgrounds. This is a
strength of the data science field as it exists now and we think this should be
nurtured.

## Books and Resources

There are some books and resources we recommend.

  - [Doing data science with R](https://r4ds.had.co.nz/) by Wickham and
    Grolemund (2017)
  - [Big magic with R: Creating learning beyond
    fear](https://speakerdeck.com/apreshill/big-magic-with-r-creative-learning-beyond-fear)
    by Hill (2017)

## Courses and Communities

There are also some courses and online communities we recommend.

  - [\#r4ds](https://medium.com/@kierisi/r4ds-the-next-iteration-d51e0a1b0b82)
    (see a talk at rstudio::conf()
    [here](https://resources.rstudio.com/rstudio-conf-2019/r4ds-online-learning-community-improvements-to-self-taught-data-science-and-the-critical-need-for-diversity-equity-and-inclusion-in-data-science-education)
    by Maegan (2019))
  - [Data science for social scientists](http://datascience.tntlab.org/) by
    Landers (2019)
  - [University of Oregon Data Science Specialization for the College of
    Education](https://github.com/uo-datasci-specialization) by Anderson (2019)

<!--chapter:end:15-learning-more.Rmd-->

